#!/usr/bin/env python3
"""é‡è¤‡ãƒã‚§ãƒƒã‚¯æ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

æ—¢å­˜ã®GitHub Issues (#171-175) ã¨åŒã˜è¨˜äº‹ã‚’å†åº¦å–å¾—ã—ã€
é‡è¤‡ãƒã‚§ãƒƒã‚¯ãŒæ­£ã—ãæ©Ÿèƒ½ã™ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¾ã™ã€‚
"""

import json
import subprocess
import sys
from dataclasses import dataclass
from pathlib import Path


@dataclass
class FeedItem:
    """RSSãƒ•ã‚£ãƒ¼ãƒ‰è¨˜äº‹"""

    title: str
    link: str
    published: str
    summary: str = ""


@dataclass
class GitHubIssue:
    """GitHub Issue"""

    number: int
    title: str
    url: str  # GitHub Issueã®URL
    article_url: str  # è¨˜äº‹ã®URL


def load_filter_config() -> dict:
    """ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    config_path = Path("data/config/finance-news-filter.json")
    with open(config_path) as f:
        return json.load(f)


def load_rss_items() -> list[FeedItem]:
    """RSSãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è¨˜äº‹ã‚’èª­ã¿è¾¼ã‚€"""
    items = []
    rss_dir = Path("data/raw/rss")

    # å„ãƒ•ã‚£ãƒ¼ãƒ‰ã®itemsãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    for items_file in rss_dir.glob("*/items.json"):
        with open(items_file) as f:
            data = json.load(f)
            # ãƒ‡ãƒ¼ã‚¿ã¯ {"version": "1.0", "feed_id": "...", "items": [...]} ã®æ§‹é€ 
            for item in data.get("items", []):
                items.append(
                    FeedItem(
                        title=item.get("title", ""),
                        link=item.get("link", ""),
                        published=item.get("published", ""),
                        summary=item.get("summary") or "",
                    )
                )

    return items


def get_existing_issues() -> list[GitHubIssue]:
    """æ—¢å­˜ã®GitHub Issueã‚’å–å¾—"""
    issues = []

    # Issue #171-175ã‚’å€‹åˆ¥ã«å–å¾—
    for issue_number in range(171, 176):
        try:
            result = subprocess.run(  # nosec B607
                [
                    "gh",
                    "issue",
                    "view",
                    str(issue_number),
                    "--repo",
                    "YH-05/finance",
                    "--json",
                    "number,title,url,body",
                ],
                capture_output=True,
                text=True,
                check=True,
            )

            issue_data = json.loads(result.stdout)

            # body ã‹ã‚‰è¨˜äº‹URLã‚’æŠ½å‡º
            body = issue_data.get("body", "")
            article_url = ""
            for line in body.split("\n"):
                if line.startswith("- **URL**:"):
                    article_url = line.replace("- **URL**:", "").strip()
                    break

            issues.append(
                GitHubIssue(
                    number=issue_data["number"],
                    title=issue_data["title"],
                    url=issue_data["url"],
                    article_url=article_url,
                )
            )

        except subprocess.CalledProcessError as e:
            print(f"è­¦å‘Š: Issue #{issue_number} ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    return issues


def calculate_title_similarity(title1: str, title2: str) -> float:
    """ã‚¿ã‚¤ãƒˆãƒ«ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
    words1 = set(title1.lower().split())
    words2 = set(title2.lower().split())

    if not words1 or not words2:
        return 0.0

    common = words1.intersection(words2)
    total = words1.union(words2)

    return len(common) / len(total)


def is_duplicate(
    item: FeedItem, existing_issues: list[GitHubIssue], threshold: float
) -> tuple[bool, str]:
    """é‡è¤‡ãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œ

    Returns:
        (is_duplicate, reason)
    """
    for issue in existing_issues:
        # URLå®Œå…¨ä¸€è‡´ãƒã‚§ãƒƒã‚¯
        if item.link == issue.article_url:
            return True, f"URLå®Œå…¨ä¸€è‡´: Issue #{issue.number}"

        # ã‚¿ã‚¤ãƒˆãƒ«é¡ä¼¼åº¦ãƒã‚§ãƒƒã‚¯
        similarity = calculate_title_similarity(item.title, issue.title)
        if similarity >= threshold:
            return (
                True,
                f"ã‚¿ã‚¤ãƒˆãƒ«é¡ä¼¼åº¦ {similarity:.2f} (é–¾å€¤: {threshold}): Issue #{issue.number}",
            )

    return False, ""


def matches_financial_keywords(item: FeedItem, filter_config: dict) -> bool:
    """é‡‘èã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ãƒãƒƒãƒã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
    text = f"{item.title} {item.summary}".lower()

    # é‡‘èã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
    include_keywords = filter_config["keywords"]["include"]

    match_count = 0
    for category, keywords in include_keywords.items():
        for keyword in keywords:
            if keyword.lower() in text:
                match_count += 1

    # æœ€ä½ãƒãƒƒãƒæ•°ãƒã‚§ãƒƒã‚¯
    min_matches = filter_config["filtering"]["min_keyword_matches"]
    return match_count >= min_matches


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=" * 80)
    print("é‡è¤‡ãƒã‚§ãƒƒã‚¯æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ")
    print("=" * 80)
    print()

    # 1. ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è¨­å®šèª­ã¿è¾¼ã¿
    print("[1] ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿")
    filter_config = load_filter_config()
    threshold = filter_config["filtering"]["title_similarity_threshold"]
    print(f"    âœ“ é¡ä¼¼åº¦é–¾å€¤: {threshold}")
    print()

    # 2. æ—¢å­˜Issueå–å¾—
    print("[2] æ—¢å­˜GitHub Issueå–å¾—ï¼ˆ#171-175ï¼‰")
    existing_issues = get_existing_issues()
    print(f"    âœ“ å–å¾—ä»¶æ•°: {len(existing_issues)}ä»¶")
    for issue in existing_issues:
        print(f"      - Issue #{issue.number}: {issue.title}")
        print(f"        URL: {issue.article_url}")
    print()

    # 3. RSSè¨˜äº‹å–å¾—
    print("[3] RSSãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è¨˜äº‹ã‚’å–å¾—")
    all_items = load_rss_items()
    print(f"    âœ“ ç·è¨˜äº‹æ•°: {len(all_items)}ä»¶")
    print()

    # 4. ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    print("[4] é‡‘èã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°")
    filtered_items = [
        item for item in all_items if matches_financial_keywords(item, filter_config)
    ]
    print(f"    âœ“ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œ: {len(filtered_items)}ä»¶")
    print()

    # 5. ä¸Šä½5ä»¶ã‚’é¸æŠ
    print("[5] ä¸Šä½5ä»¶ã‚’é¸æŠ")
    top_items = filtered_items[:5]
    print(f"    âœ“ é¸æŠä»¶æ•°: {len(top_items)}ä»¶")
    print()

    # 6. é‡è¤‡ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ
    print("[6] é‡è¤‡ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ")
    print()

    duplicates = []
    new_items = []

    for i, item in enumerate(top_items, 1):
        is_dup, reason = is_duplicate(item, existing_issues, threshold)

        if is_dup:
            duplicates.append((item, reason))
            print(f"    [{i}] ğŸ”´ é‡è¤‡æ¤œå‡º")
            print(f"        ã‚¿ã‚¤ãƒˆãƒ«: {item.title}")
            print(f"        URL: {item.link}")
            print(f"        ç†ç”±: {reason}")
        else:
            new_items.append(item)
            print(f"    [{i}] ğŸŸ¢ æ–°è¦è¨˜äº‹")
            print(f"        ã‚¿ã‚¤ãƒˆãƒ«: {item.title}")
            print(f"        URL: {item.link}")
        print()

    # 7. çµæœã‚µãƒãƒªãƒ¼
    print("=" * 80)
    print("ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼")
    print("=" * 80)
    print(f"å¯¾è±¡è¨˜äº‹æ•°: {len(top_items)}ä»¶")
    print(f"é‡è¤‡æ¤œå‡ºæ•°: {len(duplicates)}ä»¶")
    print(f"æ–°è¦è¨˜äº‹æ•°: {len(new_items)}ä»¶")
    print()

    if duplicates:
        print("ã€é‡è¤‡æ¤œå‡ºã•ã‚ŒãŸè¨˜äº‹ã€‘")
        for item, reason in duplicates:
            print(f"  - {item.title}")
            print(f"    URL: {item.link}")
            print(f"    ç†ç”±: {reason}")
        print()

    if new_items:
        print("ã€æ–°è¦è¨˜äº‹ã€‘")
        for item in new_items:
            print(f"  - {item.title}")
            print(f"    URL: {item.link}")
        print()

    # æœŸå¾…çµæœã¨ã®æ¯”è¼ƒ
    print("=" * 80)
    print("æœŸå¾…çµæœã¨ã®æ¯”è¼ƒ")
    print("=" * 80)
    expected_duplicates = 5
    expected_new = 0

    if len(duplicates) == expected_duplicates and len(new_items) == expected_new:
        print("âœ… ãƒ†ã‚¹ãƒˆæˆåŠŸ: æœŸå¾…é€šã‚Šã®çµæœã§ã™")
        print(f"   - é‡è¤‡æ¤œå‡ºæ•°: {len(duplicates)}ä»¶ï¼ˆæœŸå¾…: {expected_duplicates}ä»¶ï¼‰")
        print(f"   - æ–°è¦è¨˜äº‹æ•°: {len(new_items)}ä»¶ï¼ˆæœŸå¾…: {expected_new}ä»¶ï¼‰")
        return 0
    else:
        print("âŒ ãƒ†ã‚¹ãƒˆå¤±æ•—: æœŸå¾…ã¨ç•°ãªã‚‹çµæœã§ã™")
        print(f"   - é‡è¤‡æ¤œå‡ºæ•°: {len(duplicates)}ä»¶ï¼ˆæœŸå¾…: {expected_duplicates}ä»¶ï¼‰")
        print(f"   - æ–°è¦è¨˜äº‹æ•°: {len(new_items)}ä»¶ï¼ˆæœŸå¾…: {expected_new}ä»¶ï¼‰")
        return 1


if __name__ == "__main__":
    sys.exit(main())
