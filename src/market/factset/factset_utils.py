"""
factset_utils.py
"""

import contextlib
import os
import re
import sqlite3
import time
import warnings
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Callable

import numpy as np
import openpyxl
import pandas as pd
import yaml
from tqdm import tqdm

from utils_core.logging import get_logger
from utils_core.settings import load_project_env

# Legacy imports - these modules have been moved/removed
# import src.database_utils as db_utils
# import src.ROIC_make_data_files_ver2 as roic_utils

logger = get_logger(__name__)

warnings.simplefilter("ignore")


# ============================================================================================
# SQLã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³å¯¾ç­–ç”¨ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
# ============================================================================================
def _validate_sql_identifier(name: str) -> str:
    """
    SQLè­˜åˆ¥å­ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«åãƒ»ã‚«ãƒ©ãƒ åï¼‰ãŒå®‰å…¨ã§ã‚ã‚‹ã“ã¨ã‚’æ¤œè¨¼ã™ã‚‹ã€‚

    SQLiteã§ã¯è­˜åˆ¥å­ã‚’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã§ããªã„ãŸã‚ã€
    ã“ã®é–¢æ•°ã§è­˜åˆ¥å­ãŒå®‰å…¨ãªå½¢å¼ã§ã‚ã‚‹ã“ã¨ã‚’ä¿è¨¼ã™ã‚‹ã€‚

    Parameters
    ----------
    name : str
        æ¤œè¨¼ã™ã‚‹SQLè­˜åˆ¥å­

    Returns
    -------
    str
        æ¤œè¨¼æ¸ˆã¿ã®è­˜åˆ¥å­ï¼ˆå…¥åŠ›ã¨åŒã˜å€¤ï¼‰

    Raises
    ------
    ValueError
        è­˜åˆ¥å­ãŒä¸æ­£ãªå½¢å¼ã®å ´åˆ
    """
    # ç©ºæ–‡å­—åˆ—ãƒã‚§ãƒƒã‚¯
    if not name or not name.strip():
        raise ValueError("SQLè­˜åˆ¥å­ã¯ç©ºã«ã§ãã¾ã›ã‚“")

    # è¨±å¯ã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³: è‹±æ•°å­—ã€ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã€ãƒã‚¤ãƒ•ãƒ³ã€ãƒ‰ãƒƒãƒˆ
    # ï¼ˆFactSetã®ãƒ†ãƒ¼ãƒ–ãƒ«åãƒ»ã‚«ãƒ©ãƒ åã§ä½¿ç”¨ã•ã‚Œã‚‹æ–‡å­—ï¼‰
    pattern = r"^[a-zA-Z_][a-zA-Z0-9_\-\.]*$"
    if not re.match(pattern, name):
        raise ValueError(
            f"SQLè­˜åˆ¥å­ã«ä¸æ­£ãªæ–‡å­—ãŒå«ã¾ã‚Œã¦ã„ã¾ã™: {name!r}. "
            f"è¨±å¯ã•ã‚Œã‚‹å½¢å¼: è‹±å­—ã§å§‹ã¾ã‚Šã€è‹±æ•°å­—ãƒ»ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ãƒ»ãƒã‚¤ãƒ•ãƒ³ãƒ»ãƒ‰ãƒƒãƒˆã®ã¿"
        )

    # SQLã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨ã®è¡çªãƒã‚§ãƒƒã‚¯ï¼ˆä¸»è¦ãªã‚‚ã®ã ã‘ï¼‰
    sql_keywords = {
        "SELECT",
        "INSERT",
        "UPDATE",
        "DELETE",
        "DROP",
        "CREATE",
        "ALTER",
        "TABLE",
        "FROM",
        "WHERE",
        "AND",
        "OR",
        "NOT",
        "NULL",
        "TRUE",
        "FALSE",
    }
    if name.upper() in sql_keywords:
        raise ValueError(f"SQLè­˜åˆ¥å­ãŒSQLã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨è¡çªã—ã¦ã„ã¾ã™: {name}")

    return name


# ============================================================================================
def split_and_save_dataframe(
    df_all: pd.DataFrame, n_splits: int, base_dir: Path, base_filename: str, **kwargs
):
    """
    ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æŒ‡å®šã•ã‚ŒãŸæ•°ã«åˆ†å‰²ã—ã€ãã‚Œãã‚Œã‚’Parquetå½¢å¼ã§ä¿å­˜ã™ã‚‹é–¢æ•°ã€‚

    Args:
        df_all (pd.DataFrame): åˆ†å‰²å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã€‚
        n_splits (int): åˆ†å‰²ã™ã‚‹æ•°ã€‚1ä»¥ä¸Šã®æ•´æ•°ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
        base_dir (Path): ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã™ã‚‹åŸºæœ¬ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (e.g., FACTSET_DATA_DIR / "Index_Constituents")ã€‚
        base_filename (str): ãƒ•ã‚¡ã‚¤ãƒ«åã®ãƒ™ãƒ¼ã‚¹éƒ¨åˆ† (e.g., "Index_Constituents_with_Factset_code-compressed-")ã€‚
        **kwargs: pd.to_parquetã«æ¸¡ã™ãã®ä»–ã®å¼•æ•° (e.g., compression="zstd", index=False)ã€‚

    Returns:
        list: ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆã€‚
    """
    if n_splits <= 0:
        raise ValueError("n_splits ã¯ 1 ä»¥ä¸Šã®æ•´æ•°ã§ãªã‘ã‚Œã°ãªã‚Šã¾ã›ã‚“ã€‚")

    total_rows = len(df_all)
    saved_files = []

    # åˆ†å‰²ç‚¹ã‚’è¨ˆç®—
    # np.linspaceã‚’ä½¿ç”¨ã—ã¦ã€0ã‹ã‚‰total_rowsã¾ã§ã® n_splits + 1 å€‹ã®ç­‰é–“éš”ãªæ•´æ•°ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨ˆç®—
    indices = np.linspace(0, total_rows, n_splits + 1, dtype=int)

    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®åˆ†å‰²ã¨ä¿å­˜
    for i in range(n_splits):
        # åˆ†å‰²ç¯„å›²ã®é–‹å§‹ã¨çµ‚äº†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        start_idx = indices[i]
        end_idx = indices[i + 1]

        # ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º
        df_split = df_all.iloc[start_idx:end_idx]

        # ãƒ•ã‚¡ã‚¤ãƒ«åã¨ãƒ‘ã‚¹ã®è¨­å®š
        # ä¾‹: Index_Constituents_with_Factset_code-compressed-1.parquet
        file_name = f"{base_filename}{i + 1}.parquet"
        file_path = base_dir / file_name

        # Parquetå½¢å¼ã§ä¿å­˜
        df_split.to_parquet(file_path, **kwargs)
        saved_files.append(file_path)
        logger.info(
            "Split file saved",
            progress=f"{i + 1}/{n_splits}",
            file=file_name,
            rows=len(df_split),
        )

    return saved_files


# ============================================================================================
def load_bpm_and_export_factset_code_file(
    start_date: str, end_date: str, index_dir: list[Path]
):
    """
    BPM ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸ Index æ§‹æˆéŠ˜æŸ„ã® paruqet ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿å–ã‚‹

    -   (Universe name)_Constituents.parquet ã‹ã‚‰ Factset ã® P_SYMBOL ã¨ FG_COMPANY_NAME ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‚¨ã‚¯ã‚»ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã€‚
    -   ãã®å¾Œã€ã™ã¹ã¦ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ã¾ã¨ã‚ã¦ parquet ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã€‚
    """

    # ãƒ•ã‚©ãƒ«ãƒ€
    load_project_env()
    bpm_data_dir = os.environ.get("BPM_DATA_DIR")
    if bpm_data_dir is None:
        raise ValueError("BPM_DATA_DIR environment variable not set")
    BPM_DATA_DIR = Path(bpm_data_dir)

    bpm_src_dir = os.environ.get("BPM_SRC_DIR")
    if bpm_src_dir is None:
        raise ValueError("BPM_SRC_DIR environment variable not set")
    BPM_SRC_DIR = Path(bpm_src_dir)

    src_dir_str = os.environ.get("SRC_DIR")
    if src_dir_str is None:
        raise ValueError("SRC_DIR environment variable not set")
    src_dir = Path(src_dir_str)

    with open(src_dir / "BPM_Index-code-map.yaml", encoding="utf-8") as f:
        bpm_code_map = yaml.safe_load(f)
        bpm_name_to_code = {value: key for key, value in bpm_code_map.items()}

    dfs = []
    for dir in index_dir:
        parquet_file = dir / f"{dir.name}_Constituents.parquet"
        df = (
            pd.read_parquet(parquet_file)
            .query("date>=@start_date and date<=@end_date")
            .assign(
                Universe=dir.name,
                Universe_code_BPM=bpm_name_to_code[dir.name],
            )
        )
        dfs.append(df)

    df = pd.concat(dfs, ignore_index=True)
    df["Weight (%)"] = df["Weight (%)"].astype(float)
    df["SEDOL"] = df["SEDOL"].astype(str).str.zfill(7)

    # å…¨æ§‹æˆéŠ˜æŸ„ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    output_path = BPM_DATA_DIR / "Index_Constituents.parquet"
    df.to_parquet(
        output_path,
        index=False,
    )
    logger.info("BPM constituents exported", path=str(output_path))

    # Factsetã‚³ãƒ¼ãƒ‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨Excelã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    df_slice = (
        df[
            [
                "Asset ID",
                "Asset ID Type",
                "Country",
                "Universe",
                "Universe_code_BPM",
                "CUSIP",
                "SEDOL",
                "ISIN",
                "CODE_JP",
            ]
        ]
        .drop_duplicates()
        .replace("N/A", np.nan)
        .fillna(np.nan)
        .dropna(
            subset=["CUSIP", "SEDOL", "ISIN", "CODE_JP"],
            how="all",
        )
        .reset_index(drop=True)
        # .assign(CODE=lambda row: row["SEDOL"])
    )
    excel_rows = (df_slice.index + 2).astype(str)
    for col, row_alphabet in {
        "CUSIP": "F",
        "SEDOL": "G",
        "ISIN": "H",
        "CODE_JP": "I",
    }.items():
        # code_col = "G"  # éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ãŒå…¥ã£ã¦ã„ã‚‹ã‚¨ã‚¯ã‚»ãƒ«ã®ã‚«ãƒ©ãƒ (å¤‰æ›´ã™ã‚‹å ´åˆã¯è¦ç¢ºèª)
        df_slice[f"FG_COMPANY_NAME_{col}"] = (
            f"=FDS({row_alphabet}" + excel_rows + ', "FG_COMPANY_NAME"' + ")"
        )  # é–¢æ•°å¼ã¯é…åˆ—ã§æ ¼ç´
        df_slice[f"P_SYMBOL_{col}"] = (
            f"=FDS({row_alphabet}" + excel_rows + ', "P_SYMBOL"' + ")"
        )  # é–¢æ•°å¼ã¯é…åˆ—ã§æ ¼ç´

    # export
    FACTSET_ROOT_DIR = Path(os.getenv("FACTSET_ROOT_DIR"))  # type: ignore
    output_excel_path = (
        FACTSET_ROOT_DIR / "Index_Constituents/Index_Constituents_Factset_code_DL.xlsx"
    )

    df_slice.to_excel(output_excel_path, index=False)
    logger.info("Factset code download Excel exported", path=str(output_excel_path))


# ============================================================================================
def unify_factset_code_data(split_save_mode: bool = False):
    """
    Factsetã‹ã‚‰Excelã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸP_SYMBOL, FG_COMPANY_NAMEã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµ±åˆã™ã‚‹é–¢æ•°.
    P_SYMBOLã¨FG_COMPANY_NAMEã¯BPMã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸSEDOL, CUSIP, ISIN, CODE_JPã‚’å¼•æ•°ã¨ã—ã¦ã€
    ãã‚Œãã‚Œãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ã‚‹.
    """

    # ãƒ•ã‚©ãƒ«ãƒ€
    load_project_env()
    BPM_DATA_DIR = Path(os.environ.get("BPM_DATA_DIR"))  # type: ignore
    FACTSET_ROOT_DIR = Path(os.environ.get("FACTSET_ROOT_DIR"))  # type: ignore

    # BPMã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«
    df_bpm = pd.read_parquet(BPM_DATA_DIR / "Index_Constituents.parquet").replace(
        "N/A", np.nan
    )

    # Universeã”ã¨ã«ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
    # Factsetã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸã‚³ãƒ¼ãƒ‰ã®ãƒ•ã‚¡ã‚¤ãƒ«
    file_list = list(
        FACTSET_ROOT_DIR.glob(
            "Index_Constituents/M*_Index_Constituents_Factset_code_DL.csv"
        )
    )

    universe_code_bpm = [
        s.name.replace("_Index_Constituents_Factset_code_DL.csv", "") for s in file_list
    ]

    code_type_cols = ["SEDOL", "CUSIP", "ISIN", "CODE_JP"]
    df_all = []
    for universe_code, f in zip(universe_code_bpm, file_list, strict=False):
        df_factset_uni = pd.read_csv(
            f,
            encoding="utf-8",
            dtype={"CUSIP": "str", "SEDOL": "str", "ISIN": "str", "CODE_JP": "str"},
        ).replace("N/A", np.nan)
        df_bpm_uni = df_bpm[df_bpm["Universe_code_BPM"] == universe_code]

        dfs = []

        for code_type in code_type_cols:
            drop_cols = code_type_cols.copy()
            drop_cols.remove(code_type)
            df_left = df_bpm_uni.drop(columns=drop_cols)
            df_right = df_factset_uni.drop(
                columns=drop_cols
                + [f"P_SYMBOL_{s}" for s in drop_cols]
                + [f"FG_COMPANY_NAME_{s}" for s in drop_cols]
            )
            df_merge = pd.merge(
                df_left,
                df_right,
                on=[
                    col
                    for col in df_right.columns
                    if (not col.startswith("P_SYMBOL"))
                    & (not col.startswith("FG_COMPANY_NAME"))
                ],
                how="left",
            )

            dfs.append(df_merge)

        # ãƒãƒ¼ã‚¸ã—ãŸãã‚Œãã‚Œã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‹ã‚‰å…±é€šã‚«ãƒ©ãƒ ã‚’å–å¾—ã™ã‚‹
        # æœ€åˆã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã‚«ãƒ©ãƒ åã‚’åˆæœŸã‚»ãƒƒãƒˆã¨ã—ã¦å–å¾—
        common_cols = set(dfs[0].columns)
        # ãƒªã‚¹ãƒˆå†…ã®æ®‹ã‚Šã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã‚«ãƒ©ãƒ åã¨å…±é€šéƒ¨åˆ†ã‚’è¨ˆç®—
        for df_ in dfs[1:]:
            current_cols = set(df_.columns)
            common_cols = common_cols.intersection(current_cols)
        common_cols = list(common_cols)

        df_merged = pd.DataFrame()
        for index, df_code in enumerate(dfs):
            if index == 0:
                df_merged = df_code
            else:
                df_merged = pd.merge(df_merged, df_code, on=common_cols, how="left")

        # æœ€å¾Œã«ã€P_SYMBOLã¨FG_COMPANY_NAMEã‚’ã²ã¨ã¤ã«ã¾ã¨ã‚ã‚‹ã€‚
        # CODE_JPãŒæ¬ æã—ã¦ãªã„ã‚‚ã®ã¨æ¬ æã—ã¦ã„ã‚‹ã‚‚ã®ã§ãã‚Œãã‚Œå¯¾å¿œã€‚
        df_merged_copy = df_merged.copy()
        df_code_jp_not_missing = df_merged_copy[
            ~df_merged_copy["CODE_JP"].isnull()
        ].reset_index(drop=True)
        df_code_jp_missing = df_merged_copy[
            df_merged_copy["CODE_JP"].isnull()
        ].reset_index(drop=True)

        # P_SYMBOLã¨FG_COMPANY_NAMEã¯å–å¾—å¯èƒ½ãªã‚‚ã®ã§fillnaã™ã‚‹
        df_code_jp_not_missing["P_SYMBOL"] = df_code_jp_not_missing["P_SYMBOL_CODE_JP"]
        df_code_jp_not_missing["FG_COMPANY_NAME"] = df_code_jp_not_missing[
            "FG_COMPANY_NAME_CODE_JP"
        ]

        df_code_jp_missing["P_SYMBOL"] = (
            df_code_jp_missing["P_SYMBOL_SEDOL"]
            .fillna(df_code_jp_missing["P_SYMBOL_CUSIP"])  # type: ignore[arg-type]
            .fillna(df_code_jp_missing["P_SYMBOL_ISIN"])  # type: ignore[arg-type]
        )
        df_code_jp_missing["FG_COMPANY_NAME"] = (
            df_code_jp_missing["FG_COMPANY_NAME_SEDOL"]
            .fillna(df_code_jp_missing["FG_COMPANY_NAME_CUSIP"])  # type: ignore[arg-type]
            .fillna(df_code_jp_missing["FG_COMPANY_NAME_ISIN"])  # type: ignore[arg-type]
        )

        # concat
        df_final = (
            pd.concat([df_code_jp_missing, df_code_jp_not_missing])
            .sort_values("date")
            .drop_duplicates(ignore_index=True)
        )

        # export
        df_final.to_parquet(
            FACTSET_ROOT_DIR
            / f"Index_Constituents/{universe_code}_Index_Constituents_with_Factset_code.parquet",
            index=False,
        )
        df_all.append(df_final)
        logger.info(
            "Universe constituents exported",
            universe=universe_code,
            file=f"{universe_code}_Index_Constituents_with_Factset_code.parquet",
        )
        del df_final

    # concatenate and export
    df_all = pd.concat(df_all).drop_duplicates(ignore_index=True)
    df_all.to_parquet(
        FACTSET_ROOT_DIR
        / "Index_Constituents/Index_Constituents_with_Factset_code.parquet",
        index=False,
    )
    logger.info(
        "Combined constituents exported",
        file="Index_Constituents_with_Factset_code.parquet",
    )

    if split_save_mode:
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ãŒå¤§ãã„ãŸã‚åˆ†å‰²ã—ã¦åœ§ç¸®
        saved_paths = split_and_save_dataframe(
            df_all=df_all,
            base_dir=FACTSET_ROOT_DIR / "Index_Constituents",
            n_splits=5,
            base_filename="Index_Constituents_with_Factset_code-compressed-",
            compression="zstd",
            index=False,
        )
    else:
        index_constituents_dir = FACTSET_ROOT_DIR / "Index_Constituents"
        already_exsiting_compressed_files = list(
            index_constituents_dir.glob(
                "Index_Constituents_with_Factset_code-compressed-*.parquet"
            )
        )
        saved_paths = [
            (
                index_constituents_dir
                / f"Index_Constituents_with_Factset_code-compressed-{len(already_exsiting_compressed_files) + 1}.parquet"
            )
        ]
        df_all.to_parquet(saved_paths[0], index=False, compression="zstd")

    logger.info("All split files saved", paths=[str(p) for p in saved_paths])

    del df_all
    logger.info("Universe constituents export completed")


# ============================================================================================
def create_factset_symbol_list_function(universe_code: str) -> str:
    """
    GET constituents list
    Factset formulaç”¨ã«Factset P_SYMBOLã‚’å–å¾— -> stringã«å¤‰æ›

    universe_code: BPMã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚³ãƒ¼ãƒ‰
    """

    load_project_env()
    factset_root_dir = Path(os.environ.get("FACTSET_ROOT_DIR"))  # type: ignore
    symbol_list = (
        pd.read_parquet(
            factset_root_dir
            / f"Index_Constituents/{universe_code}_Index_Constituents_with_Factset_code.parquet"
        )
        .dropna(subset=["P_SYMBOL"])
        .sort_values("P_SYMBOL")["P_SYMBOL"]
        .unique()
        .tolist()
    )
    symbol_list_str = " ".join(symbol_list)
    symbol_list_function = (
        f'^=STRING SYMBOLS=TICKER_LIST("{symbol_list_str}")'  # FQLåŸ‹ã‚è¾¼ã¿ç”¨é–¢æ•°
    )

    return symbol_list_function


# ============================================================================================
def factset_formula(item: str, year_range: str = "20AY", per: str = "M") -> str:
    """
    Factset formulaä½œæˆç”¨é–¢æ•°

    item: Factseté–¢æ•°ã®item name(ex. FF_SALES)
    per: å–å¾—é »åº¦(D: Daily, M: Monthly, Q: Quarterly, A: Annually)
    """

    period_start = "-" + year_range if not year_range.startswith("-") else year_range
    if item == "FF_ENTRPR_VAL_DAILY":
        excel_function = f'^=GET_FQL_ARRAY(SYMBOLS, "{item}({period_start}, 0, {per},, USD, ""DIL"")")'
    elif item == "FG_PRICE":
        excel_function = (
            f'^=GET_FQL_ARRAY(SYMBOLS, "FG_PRICE({period_start}, 0, {per})")'
        )
    else:
        excel_function = f'^=GET_FQL_ARRAY(SYMBOLS, "AVAIL({item}(QTR_R, {period_start}, 0, {per},,USD), {item}(SEMI_R, {period_start}, 0, {per},,USD), {item}(ANN_R, {period_start}, 0, {per},,USD))")'
    return excel_function


# ============================================================================================
def implement_factset_formulas(universe_code: str, year_range: str = "20AY") -> None:
    """
    Universeã‚’BPMã®ã‚³ãƒ¼ãƒ‰ã§æŒ‡å®šã—ã€Factsetã‹ã‚‰è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‚¨ã‚¯ã‚»ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã™ã‚‹é–¢æ•°
    universe_code: BPMã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚³ãƒ¼ãƒ‰
    """

    # ãƒ•ã‚©ãƒ«ãƒ€
    load_project_env()
    FACTSET_ROOT_DIR = Path(os.environ.get("FACTSET_ROOT_DIR"))  # type: ignore
    FACTSET_FINANCIALS_DIR = Path(os.environ.get("FACTSET_FINANCIALS_DIR"))  # type: ignore
    #  read factset formula items
    formula_xlsx = FACTSET_ROOT_DIR / "FDS samples and Factset Formulas.xlsx"
    df = pd.read_excel(formula_xlsx, sheet_name="Sheet3")

    symbol_list_function = create_factset_symbol_list_function(
        universe_code=universe_code
    )

    logger.info(
        "Factset formula embedding started",
        universe=universe_code,
        category_count=len(df["Category"].unique()),
    )
    category_list = df["Category"].unique().tolist()
    logger.debug("Categories", categories=", ".join(category_list))
    for category in category_list:
        df_cat = df.loc[
            df["Category"] == category,
            ["Category", "Item", "name"],
        ]
        universe_folder = FACTSET_FINANCIALS_DIR / universe_code
        universe_folder.mkdir(exist_ok=True)

        per = "M"
        excel_file = universe_folder / f"Financials_{category}_{year_range}.xlsx"
        if category == "Price_Daily":
            per = "D"
            excel_file = universe_folder / f"Price_Daily_{year_range}.xlsx"
        elif category == "Price":
            per = "M"
            excel_file = universe_folder / f"Price_{year_range}.xlsx"

        # export to Excel file
        wb = openpyxl.Workbook()
        for i, item in enumerate(df_cat["Item"].tolist()):
            ws = wb.create_sheet(title=item, index=i)
            ws["A1"].value = "date"
            ws["A2"].value = f"^=P_DATE(-{year_range}, 0, {per})"
            ws["B1"].value = symbol_list_function
            ws["B2"].value = factset_formula(item=item, year_range=year_range, per=per)
        wb.save(excel_file)
        wb.close()

    logger.info("Factset formula embedding completed", universe=universe_code)


# ============================================================================================
def format_factset_downloaded_data(
    file_list: list[Path | str], output_folder: Path, split_save_mode: bool = False
):
    """Factsetã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸè²¡å‹™ãƒ»ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢ã—Parquetå½¢å¼ã§ä¿å­˜ã™ã‚‹ã€‚

    æŒ‡å®šã•ã‚ŒãŸExcelãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã¿ã€å„ã‚·ãƒ¼ãƒˆã®ãƒ‡ãƒ¼ã‚¿ã‚’ç¸¦æŒã¡å½¢å¼(long format)ã«å¤‰æ›ã—ã¾ã™ã€‚
    å¤‰æ›å¾Œã®ãƒ‡ãƒ¼ã‚¿ã¯å˜ä¸€ã®DataFrameã«çµåˆã•ã‚Œã€Parquetãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦å‡ºåŠ›ã•ã‚Œã¾ã™ã€‚
    ä¿å­˜ãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ã¦ã€å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯åˆ†å‰²ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ã—ã¾ã™ã€‚

    Parameters
    ----------
    file_list : list[Path | str]
        Factsetã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã‚‹Excelãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆã€‚
    output_folder : Path
        å‡ºåŠ›å…ˆã®Parquetãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã™ã‚‹ãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹ã€‚
    split_save_mode : bool, optional
        ä¿å­˜ãƒ¢ãƒ¼ãƒ‰ã‚’åˆ¶å¾¡ã™ã‚‹ãƒ•ãƒ©ã‚°, by default False.
        - `True`ã®å ´åˆ:
            1. `Financials_and_Price.parquet`ãŒå­˜åœ¨ã™ã‚Œã°ã€æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’è¿½è¨˜ã—ã¦æ›´æ–°ã—ã¾ã™ã€‚
                å­˜åœ¨ã—ãªã„å ´åˆã¯æ–°è¦ä½œæˆã—ã¾ã™ã€‚
            2. ã•ã‚‰ã«ã€ãƒ‡ãƒ¼ã‚¿ã‚’3å¹´ã”ã¨ã®æœŸé–“ã§åˆ†å‰²ã—ã€
                `Financials_and_Price-compressed-YYYY-YYYY.parquet`ã¨ã„ã†åå‰ã®
                åœ§ç¸®ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ã—ã¾ã™ã€‚
        - `False`ã®å ´åˆ:
            å…¨æœŸé–“ã®ãƒ‡ãƒ¼ã‚¿ã‚’å˜ä¸€ã®åœ§ç¸®ãƒ•ã‚¡ã‚¤ãƒ«
            `Financials_and_Price-compressed-YYYYMMDD_YYYYMMDD.parquet`
            ã¨ã—ã¦ä¿å­˜ã—ã¾ã™ã€‚

    Returns
    -------
    None
        ã“ã®é–¢æ•°ã¯å€¤ã‚’è¿”ã•ãšã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡ºåŠ›ã—ã¾ã™ã€‚

    """

    df_all = pd.DataFrame()
    all_melted_dfs = []

    for f in tqdm(file_list):
        xls = pd.ExcelFile(f, engine="calamine")
        sheet_names = [
            s for s in xls.sheet_names if s not in ["Sheet", "FF_ENTRPR_VAL_DAILY"]
        ]
        melted_df = [
            pd.melt(
                pd.read_excel(f, sheet_name=sheet_name).dropna(
                    how="all", axis=0, ignore_index=True
                ),
                id_vars="date",  # type: ignore
                var_name="P_SYMBOL",
                value_name="value",
            ).assign(variable=sheet_name)
            for sheet_name in sheet_names
        ]
        all_melted_dfs.extend(melted_df)

    df_all = pd.concat(all_melted_dfs, ignore_index=True)
    df_all["date"] = pd.to_datetime(df_all["date"])
    df_all["value"] = df_all["value"].astype(float)

    # export
    if split_save_mode:
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ãŒå¤§ãã„ãŸã‚åˆ†å‰²ã—ã¦åœ§ç¸®
        already_exsiting_compressed_file = (
            output_folder / "Financials_and_Price.parquet"
        )
        if already_exsiting_compressed_file.exists():
            df_existed = pd.read_parquet(already_exsiting_compressed_file)
            df_all = pd.concat([df_existed, df_all]).drop_duplicates(ignore_index=True)
            df_all.to_parquet(
                already_exsiting_compressed_file, index=False, compression="zstd"
            )
            logger.info("File exported", path=str(already_exsiting_compressed_file))
        else:
            output_path = output_folder / "Financials_and_Price.parquet"
            df_all.to_parquet(output_path, index=False, compression="zstd")
            logger.info("File exported", path=str(output_path))

        # åˆ†å‰²ã—ã¦export
        start_year_list = np.arange(pd.to_datetime(df_all["date"]).min().year, 2025, 3)
        for start, end in zip(start_year_list, start_year_list + 2, strict=False):
            df_slice = df_all[
                (df_all["date"].dt.year >= start) & (df_all["date"].dt.year <= end)
            ]
            output_path = (
                output_folder / f"Financials_and_Price-compressed-{start}-{end}.parquet"
            )
            df_slice.to_parquet(output_path, index=False, compression="zstd")
            logger.info("File exported", path=str(output_path))

    else:
        start_date = df_all["date"].min().strftime("%Y%m%d")
        end_date = df_all["date"].max().strftime("%Y%m%d")
        output_path = (
            output_folder
            / f"Financials_and_Price-compressed-{start_date}_{end_date}.parquet"
        )
        df_all.to_parquet(output_path, index=False, compression="zstd")
        logger.info("File exported", path=str(output_path))


# ============================================================================================
# def store_to_database(
#     df: pd.DataFrame,
#     db_path: Path,
#     table_name: str,
#     unique_cols: list[str] = ["date", "P_SYMBOL", "variable"],
#     verbose: bool = True,
#     replace_on_conflict: bool = False,
# ):
#     """
#     Pandas DataFrameã‚’SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ›¸ãè¾¼ã‚€ã€‚
#     æ—¢å­˜ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã¨é‡è¤‡ã™ã‚‹è¡Œ(date, P_SYMBOL, value, variable ã®çµ„ã¿åˆã‚ã›ãŒä¸€è‡´ã™ã‚‹è¡Œ)
#     ã¯è¿½åŠ ã—ãªã„ã€‚

#     Args:
#         df (pd.DataFrame): æ›¸ãè¾¼ã¿ãŸã„ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ (date, P_SYMBOL, value, variable ã®ã‚«ãƒ©ãƒ ã‚’æŒã¤)ã€‚
#         db_path (str): æ¥ç¶šã™ã‚‹SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã€‚
#         table_name (str): æ›¸ãè¾¼ã¿å…ˆã®ãƒ†ãƒ¼ãƒ–ãƒ«åã€‚
#         unique_cols ([str]): ä¸€æ„æ€§ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹ã‚«ãƒ©ãƒ 
#     """

#     # å¿…é ˆã‚«ãƒ©ãƒ ã®ãƒã‚§ãƒƒã‚¯
#     if not all(col in df.columns for col in unique_cols):
#         raise ValueError(
#             f"ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«ã¯å¿…é ˆã®ã‚«ãƒ©ãƒ  {unique_cols} ã®å…¨ã¦ãŒå«ã¾ã‚Œã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚"
#         )

#     # 1. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ¥ç¶š
#     conn = sqlite3.connect(db_path)
#     conn.execute(
#         f"""
#             CREATE TABLE IF NOT EXISTS {table_name} (
#                 date TEXT,
#                 P_SYMBOL TEXT,
#                 variable TEXT,
#                 value REAL,
#                 PRIMARY KEY ({",".join(unique_cols)})
#                 )
#         """
#     )
#     # 2. æ—¢å­˜ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ä¸€æ„æ€§ãƒã‚§ãƒƒã‚¯ã«å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€é‡è¤‡è¡Œã‚’é™¤å¤–
#     try:
#         # ãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆã€æ—¢å­˜ã®è¤‡åˆã‚­ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
#         select_cols = ", ".join(unique_cols)
#         existing_df = pd.read_sql(f"SELECT {select_cols} FROM {table_name}", conn)

#         # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã® 'date' ã‚«ãƒ©ãƒ ã‚‚ datetime å‹ã«å¤‰æ›ã™ã‚‹
#         # (unique_cols ã« 'date' ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿)
#         if "date" in unique_cols and "date" in existing_df.columns:
#             existing_df["date"] = pd.to_datetime(existing_df["date"])

#         # é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ã«æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã¨æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ¼ã‚¸
#         merged_df = pd.merge(
#             df.drop_duplicates(subset=unique_cols),  # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿å´ã‚‚è‡ªèº«ã®é‡è¤‡ã‚’é™¤å»
#             existing_df,
#             on=unique_cols,
#             how="left",
#             indicator=True,
#         )

#         # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œã¦ã„ãªã„è¡Œ ('left_only') ã®ã¿ã‚’é¸æŠ
#         df_to_add = merged_df[merged_df["_merge"] == "left_only"].drop(
#             columns=["_merge"]
#         )

#         if df_to_add.empty:
#             print(
#                 f"ãƒ†ãƒ¼ãƒ–ãƒ« '{table_name}' ã«è¿½åŠ ã™ã¹ãæ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚"
#             )
#             conn.close()
#             return

#         if verbose:
#             print(
#                 f"æ—¢å­˜ã® {len(existing_df)} è¡Œã¨ã®é‡è¤‡ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¾ã—ãŸã€‚{len(df_to_add)} è¡Œã‚’æ–°ãŸã«è¿½åŠ ã—ã¾ã™ã€‚"
#             )

#     except pd.io.sql.DatabaseError:  # type: ignore
#         # ãƒ†ãƒ¼ãƒ–ãƒ«ãŒã¾ã å­˜åœ¨ã—ãªã„å ´åˆã€å…¨ã¦ã®è¡Œã‚’è¿½åŠ 
#         df_to_add = df.drop_duplicates(subset=unique_cols)  # è‡ªèº«ã®é‡è¤‡ã¯é™¤å»
#         if verbose:
#             print(
#                 f"ãƒ†ãƒ¼ãƒ–ãƒ« '{table_name}' ã¯å­˜åœ¨ã—ã¾ã›ã‚“ã€‚æ–°ã—ã„ãƒ†ãƒ¼ãƒ–ãƒ«ã¨ã—ã¦ã€ã™ã¹ã¦ã® {len(df_to_add)} è¡Œã‚’è¿½åŠ ã—ã¾ã™ã€‚"
#             )

#     # 3. ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªè¡Œã ã‘ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ›¸ãè¾¼ã¿
#     df_to_add.to_sql(table_name, conn, if_exists="append", index=False)

#     # 4. æ¥ç¶šã‚’é–‰ã˜ã‚‹
#     conn.close()
#     if verbose:
#         print(f"  -> {table_name}: ãƒ‡ãƒ¼ã‚¿ã®æ›¸ãè¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")


# ============================================================================================
def store_to_database(
    df: pd.DataFrame,
    db_path: Path,
    table_name: str,
    unique_cols: list[str] | None = None,
    verbose: bool = True,
    on_duplicate: str = "skip",  # "skip" ã¾ãŸã¯ "update"
):
    """
    Pandas DataFrameã‚’SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ›¸ãè¾¼ã‚€ã€‚

    Args:
        df (pd.DataFrame): æ›¸ãè¾¼ã¿ãŸã„ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ (date, P_SYMBOL, value, variable ã®ã‚«ãƒ©ãƒ ã‚’æŒã¤)ã€‚
        db_path (str): æ¥ç¶šã™ã‚‹SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã€‚
        table_name (str): æ›¸ãè¾¼ã¿å…ˆã®ãƒ†ãƒ¼ãƒ–ãƒ«åã€‚
        unique_cols ([str]): ä¸€æ„æ€§ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹ã‚«ãƒ©ãƒ 
        on_duplicate (str): é‡è¤‡æ™‚ã®å‹•ä½œ - "skip" (ã‚¹ã‚­ãƒƒãƒ—) ã¾ãŸã¯ "update" (ä¸Šæ›¸ã)
    """
    if unique_cols is None:
        unique_cols = ["date", "P_SYMBOL", "variable"]

    # å¿…é ˆã‚«ãƒ©ãƒ ã®ãƒã‚§ãƒƒã‚¯
    if not all(col in df.columns for col in unique_cols):
        raise ValueError(
            f"ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«ã¯å¿…é ˆã®ã‚«ãƒ©ãƒ  {unique_cols} ã®å…¨ã¦ãŒå«ã¾ã‚Œã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚"
        )

    # ãƒ†ãƒ¼ãƒ–ãƒ«åã¨ã‚«ãƒ©ãƒ åã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆSQLã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³å¯¾ç­–ï¼‰
    _validate_sql_identifier(table_name)
    for col in unique_cols:
        _validate_sql_identifier(col)

    # 1. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ¥ç¶š
    conn = sqlite3.connect(db_path)
    # nosec B608 - table_name, unique_cols ã¯ _validate_sql_identifier() ã§æ¤œè¨¼æ¸ˆã¿
    conn.execute(
        f"""
            CREATE TABLE IF NOT EXISTS {table_name} (
                date TEXT,
                P_SYMBOL TEXT,
                variable TEXT,
                value REAL,
                PRIMARY KEY ({",".join(unique_cols)})
            )
        """
    )

    # 2. æ—¢å­˜ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ä¸€æ„æ€§ãƒã‚§ãƒƒã‚¯ã«å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€é‡è¤‡è¡Œã‚’é™¤å¤–
    try:
        # ãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆã€æ—¢å­˜ã®è¤‡åˆã‚­ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        # table_name, unique_cols ã¯ _validate_sql_identifier() ã§æ¤œè¨¼æ¸ˆã¿
        select_cols = ", ".join(unique_cols)
        existing_df = pd.read_sql(f"SELECT {select_cols} FROM {table_name}", conn)  # nosec B608

        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã® 'date' ã‚«ãƒ©ãƒ ã‚‚ datetime å‹ã«å¤‰æ›ã™ã‚‹
        if "date" in unique_cols and "date" in existing_df.columns:
            existing_df["date"] = pd.to_datetime(existing_df["date"])

        # é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ã«æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã¨æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ¼ã‚¸
        merged_df = pd.merge(
            df.drop_duplicates(subset=unique_cols),
            existing_df,
            on=unique_cols,
            how="left",
            indicator=True,
        )

        # æ–°è¦ãƒ‡ãƒ¼ã‚¿ã¨é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†é›¢
        df_to_add = merged_df[merged_df["_merge"] == "left_only"].drop(
            columns=["_merge"]
        )
        df_to_update = merged_df[merged_df["_merge"] == "both"].drop(columns=["_merge"])

        if on_duplicate == "update" and not df_to_update.empty:
            # é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã—ã¦ã‹ã‚‰è¿½åŠ ï¼ˆä¸Šæ›¸ãï¼‰
            # table_name, unique_cols ã¯ _validate_sql_identifier() ã§æ¤œè¨¼æ¸ˆã¿
            delete_count = 0
            for _, row in df_to_update.iterrows():
                conditions = " AND ".join([f"{col} = ?" for col in unique_cols])
                values = tuple(
                    str(row[col]) if isinstance(row[col], pd.Timestamp) else row[col]
                    for col in unique_cols
                )
                conn.execute(f"DELETE FROM {table_name} WHERE {conditions}", values)  # nosec B608
                delete_count += 1

            conn.commit()  # DELETEã‚’ç¢ºå®š

            if verbose:
                logger.debug(
                    "Duplicate rows deleted for overwrite", delete_count=delete_count
                )

            # é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ å¯¾è±¡ã«å«ã‚ã‚‹
            df_to_add = pd.concat([df_to_add, df_to_update], ignore_index=True)

        if df_to_add.empty:
            logger.info("No new data to add, skipping", table=table_name)
            conn.close()
            return

        if verbose:
            if on_duplicate == "update":
                logger.debug(
                    "Rows to add",
                    table=table_name,
                    total=len(df_to_add),
                    overwritten=len(df_to_update),
                )
            else:
                logger.debug(
                    "Deduplication check completed",
                    table=table_name,
                    existing_rows=len(existing_df),
                    new_rows=len(df_to_add),
                )

    except pd.io.sql.DatabaseError:  # type: ignore
        # ãƒ†ãƒ¼ãƒ–ãƒ«ãŒã¾ã å­˜åœ¨ã—ãªã„å ´åˆã€å…¨ã¦ã®è¡Œã‚’è¿½åŠ 
        df_to_add = df.drop_duplicates(subset=unique_cols)
        if verbose:
            logger.debug(
                "Table does not exist, creating new",
                table=table_name,
                rows=len(df_to_add),
            )

    # 3. ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªè¡Œã ã‘ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ›¸ãè¾¼ã¿
    df_to_add.to_sql(table_name, conn, if_exists="append", index=False)
    conn.commit()  # INSERTã‚’ç¢ºå®š

    # 4. æ¥ç¶šã‚’é–‰ã˜ã‚‹
    conn.close()
    if verbose:
        logger.info("Data written to database", table=table_name)


# ============================================================================================
# WALãƒ¢ãƒ¼ãƒ‰æœ‰åŠ¹åŒ–é–¢æ•°
# ============================================================================================


def enable_wal_mode(db_path: Path, verbose: bool = True) -> None:
    """
    SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã§WALãƒ¢ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹åŒ–

    WAL(Write-Ahead Logging)ãƒ¢ãƒ¼ãƒ‰ã«ã‚ˆã‚Šã€èª­ã¿è¾¼ã¿ã¨æ›¸ãè¾¼ã¿ã®ä¸¦è¡Œæ€§ãŒå‘ä¸Šã—ã¾ã™ã€‚
    ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä½œæˆæ™‚ã«ä¸€åº¦ã ã‘å®Ÿè¡Œã™ã‚Œã°ã€ä»¥é™ã¯æ°¸ç¶šçš„ã«æœ‰åŠ¹ã§ã™ã€‚

    :param db_path: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    :param verbose: é€²æ—è¡¨ç¤ºãƒ•ãƒ©ã‚°
    """
    with sqlite3.connect(db_path) as conn:
        cursor = conn.cursor()

        # ç¾åœ¨ã®ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«ãƒ¢ãƒ¼ãƒ‰ã‚’ç¢ºèª
        cursor.execute("PRAGMA journal_mode")
        current_mode = cursor.fetchone()[0]

        if current_mode != "wal":
            # WALãƒ¢ãƒ¼ãƒ‰ã«å¤‰æ›´
            cursor.execute("PRAGMA journal_mode=WAL")
            new_mode = cursor.fetchone()[0]

            if verbose:
                logger.info(
                    "Journal mode changed", from_mode=current_mode, to_mode=new_mode
                )
        elif verbose:
            logger.debug("Already in WAL mode")


# ============================================================================================
# ğŸ†• ç›´åˆ—æ›¸ãè¾¼ã¿ç‰ˆ(æ¨å¥¨ãƒ»ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ­ãƒƒã‚¯å®Œå…¨å›é¿)
# ============================================================================================


def store_active_returns_batch_serial_write(
    df_active_returns: pd.DataFrame,
    return_cols: list[str],
    db_path: Path,
    benchmark_ticker: str,
    batch_size: int = 10000,
    verbose: bool = True,
) -> dict[str, Any]:
    """
    ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒªã‚¿ãƒ¼ãƒ³ã‚’ãƒãƒƒãƒä¿å­˜(ç›´åˆ—æ›¸ãè¾¼ã¿ç‰ˆãƒ»ãƒ­ãƒƒã‚¯å®Œå…¨å›é¿)

    ã“ã®é–¢æ•°ã¯ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ­ãƒƒã‚¯å•é¡Œã‚’å®Œå…¨ã«è§£æ±ºã—ã¾ã™ï¼š
    - ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†(ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã€å‹å¤‰æ›)ã¯é«˜é€Ÿã«å®Ÿè¡Œ
    - DBæ›¸ãè¾¼ã¿ã¯1æ¥ç¶šã§ç›´åˆ—åŒ–ã—ã€ãƒ­ãƒƒã‚¯ã‚’å®Œå…¨å›é¿
    - 100%ã®æˆåŠŸç‡ã‚’ä¿è¨¼

    :param df_active_returns: ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒªã‚¿ãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿(Longå½¢å¼)
    :param return_cols: å‡¦ç†å¯¾è±¡ã®ãƒªã‚¿ãƒ¼ãƒ³åˆ—åãƒªã‚¹ãƒˆ
    :param db_path: SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    :param benchmark_ticker: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚£ãƒƒã‚«ãƒ¼(é™¤å¤–ç”¨)
    :param batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º(ä¸€åº¦ã«æŒ¿å…¥ã™ã‚‹è¡Œæ•°)
    :param verbose: é€²æ—è¡¨ç¤ºãƒ•ãƒ©ã‚°
    :return: å‡¦ç†çµæœã®çµ±è¨ˆæƒ…å ±
    """
    if verbose:
        logger.info(
            "Active returns batch save started (serial write)",
            columns=len(return_cols),
            rows=len(df_active_returns),
            batch_size=batch_size,
        )

    import time

    start_time = time.time()

    # --------------------------------------------------------------------------
    # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†(å‹å¤‰æ›ã€ã‚°ãƒ«ãƒ¼ãƒ—åŒ–)
    # --------------------------------------------------------------------------
    if verbose:
        logger.debug("Data preprocessing started")

    prep_start = time.time()

    # ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ”ãƒ¼ã¨å‹å¤‰æ›
    df_opt = df_active_returns.copy()
    df_opt["value"] = df_opt["value"].astype(float)
    df_opt["date"] = pd.to_datetime(df_opt["date"])
    df_opt = df_opt.rename(columns={"symbol": "P_SYMBOL"})

    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é™¤å¤–
    df_opt = df_opt[df_opt["P_SYMBOL"] != benchmark_ticker]

    # variableåˆ—ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¦è¾æ›¸åŒ–(é«˜é€Ÿã‚¢ã‚¯ã‚»ã‚¹ç”¨)
    df_dict = {}
    for col in return_cols:
        active_return_col = col.replace("Return", "Active_Return")

        # queryã®ä»£ã‚ã‚Šã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°(100å€é«˜é€Ÿ)
        df_slice = df_opt[df_opt["variable"] == active_return_col].reset_index(
            drop=True
        )

        if not df_slice.empty:
            # è‡ªèº«ã®é‡è¤‡ã‚’é™¤å»
            df_slice = df_slice.drop_duplicates(
                subset=["date", "P_SYMBOL", "variable"], ignore_index=True
            )
            df_dict[active_return_col] = df_slice

    prep_time = time.time() - prep_start

    if verbose:
        logger.info(
            "Preprocessing completed",
            elapsed_sec=round(prep_time, 2),
            tables=len(df_dict),
        )

    # --------------------------------------------------------------------------
    # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ›¸ãè¾¼ã¿(ç›´åˆ—åŒ–ã§ãƒ­ãƒƒã‚¯å›é¿)
    # --------------------------------------------------------------------------
    results = {
        "success": [],
        "failed": [],
        "total_rows": 0,
        "prep_time": prep_time,
        "save_time": 0,
        "total_time": 0,
    }

    save_start = time.time()

    # å˜ä¸€ã®æ¥ç¶šã‚’ä½¿ç”¨(ç›´åˆ—å‡¦ç†ã§ãƒ­ãƒƒã‚¯å®Œå…¨å›é¿)
    with sqlite3.connect(db_path, timeout=30.0) as conn:
        cursor = conn.cursor()

        # é€²æ—ãƒãƒ¼
        iterator = (
            tqdm(df_dict.items(), desc="ğŸ’¾ ä¿å­˜ä¸­") if verbose else df_dict.items()
        )

        for table_name, df in iterator:
            try:
                # ãƒ†ãƒ¼ãƒ–ãƒ«åã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆSQLã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³å¯¾ç­–ï¼‰
                _validate_sql_identifier(table_name)

                # ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ(å­˜åœ¨ã—ãªã„å ´åˆ)
                # nosec B608 - table_name ã¯ _validate_sql_identifier() ã§æ¤œè¨¼æ¸ˆã¿
                cursor.execute(
                    f"""
                    CREATE TABLE IF NOT EXISTS "{table_name}" (
                        date TEXT,
                        P_SYMBOL TEXT,
                        variable TEXT,
                        value REAL,
                        PRIMARY KEY (date, P_SYMBOL, variable)
                    )
                    """
                )

                # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
                try:
                    select_cols = ", ".join(["date", "P_SYMBOL", "variable"])
                    # table_name ã¯ _validate_sql_identifier() ã§æ¤œè¨¼æ¸ˆã¿
                    query = f'SELECT {select_cols} FROM "{table_name}"'  # nosec B608
                    existing_df = pd.read_sql(query, conn)

                    # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®dateåˆ—ã‚‚å¤‰æ›
                    if "date" in existing_df.columns:
                        existing_df["date"] = pd.to_datetime(existing_df["date"])

                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                    merged_df = pd.merge(
                        df,
                        existing_df,
                        on=["date", "P_SYMBOL", "variable"],
                        how="left",
                        indicator=True,
                    )

                    df_to_add = merged_df[merged_df["_merge"] == "left_only"].drop(
                        columns=["_merge"]
                    )

                    if df_to_add.empty:
                        if verbose:
                            logger.debug(
                                "Duplicate data only, skipping", table=table_name
                            )
                        continue

                except pd.io.sql.DatabaseError:  # type: ignore
                    # ãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯å…¨ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
                    df_to_add = df

                row_count = len(df_to_add)

                # ãƒãƒƒãƒINSERTã§ä¿å­˜
                df_to_add.to_sql(
                    table_name,
                    conn,
                    if_exists="append",
                    index=False,
                    chunksize=batch_size,
                    method="multi",
                )

                conn.commit()

                results["success"].append(table_name)
                results["total_rows"] += row_count

                if verbose:
                    logger.debug("Table saved", table=table_name, rows=row_count)

            except Exception as e:
                results["failed"].append({"table": table_name, "error": str(e)})
                logger.error(
                    "Table save failed", table=table_name, error=str(e), exc_info=True
                )

                # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯
                with contextlib.suppress(BaseException):
                    conn.rollback()

    results["save_time"] = time.time() - save_start
    results["total_time"] = time.time() - start_time

    # --------------------------------------------------------------------------
    # ã‚¹ãƒ†ãƒƒãƒ—3: çµ±è¨ˆæƒ…å ±è¡¨ç¤º
    # --------------------------------------------------------------------------
    if verbose:
        success_rate = (len(results["success"]) / len(df_dict) * 100) if df_dict else 0
        throughput = (
            results["total_rows"] / results["total_time"]
            if results["total_time"] > 0
            else 0
        )
        logger.info(
            "Batch save completed",
            success_count=len(results["success"]),
            total_tables=len(df_dict),
            failed_count=len(results["failed"]),
            total_rows=results["total_rows"],
            prep_time_sec=round(results["prep_time"], 2),
            save_time_sec=round(results["save_time"], 2),
            total_time_sec=round(results["total_time"], 2),
            throughput_per_sec=round(throughput),
            success_rate_pct=round(success_rate, 1),
        )

        if results["failed"]:
            for failed in results["failed"]:
                logger.warning(
                    "Failed table", table=failed["table"], error=failed["error"]
                )

    return results


# ============================================================================================
def insert_active_returns_optimized_sqlite(
    df_active_returns: pd.DataFrame,
    return_cols: list[str],
    db_path: Path,
    benchmark_ticker: str,
    # batch_sizeã¯executemanyã®åˆ†å‰²ã«ã¯ä½¿ã‚ã‚Œãªã„ãŒã€äº’æ›æ€§ã®ãŸã‚æ®‹ã™
    batch_size: int = 10000,
    verbose: bool = True,
) -> dict[str, Any]:
    """
    ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒªã‚¿ãƒ¼ãƒ³ã‚’ãƒãƒƒãƒä¿å­˜(ç›´åˆ—æ›¸ãè¾¼ã¿ç‰ˆãƒ»ãƒ­ãƒƒã‚¯å®Œå…¨å›é¿ãƒ»DBæœ€é©åŒ–)

    - ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†(ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã€å‹å¤‰æ›)ã¯é«˜é€Ÿã«å®Ÿè¡Œ
    - DBæ›¸ãè¾¼ã¿ã¯1æ¥ç¶šã§ç›´åˆ—åŒ–ã—ã€ãƒ­ãƒƒã‚¯ã‚’å®Œå…¨å›é¿
    - æ›¸ãè¾¼ã¿é€Ÿåº¦å‘ä¸Šã®ãŸã‚ã€INSERT OR IGNOREã¨executemanyã‚’ä½¿ç”¨
    """
    if verbose:
        logger.info(
            "Active returns optimized batch save started (SQLite)",
            columns=len(return_cols),
            rows=len(df_active_returns),
            batch_size=batch_size,
        )

    start_time = time.time()

    # --------------------------------------------------------------------------
    # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†(å‹å¤‰æ›ã€ã‚°ãƒ«ãƒ¼ãƒ—åŒ–)- å¤‰æ›´ãªã—
    # --------------------------------------------------------------------------
    if verbose:
        logger.debug("Data preprocessing started")

    prep_start = time.time()

    # ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ”ãƒ¼ã¨å‹å¤‰æ›
    df_opt = df_active_returns.copy()
    df_opt["value"] = df_opt["value"].astype(float)
    df_opt["date"] = pd.to_datetime(df_opt["date"]).dt.strftime(
        "%Y-%m-%d %H:%M:%S"
    )  # TEXTå½¢å¼ã«å¤‰æ›
    df_opt = df_opt.rename(columns={"symbol": "P_SYMBOL"})

    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é™¤å¤–
    df_opt = df_opt[df_opt["P_SYMBOL"] != benchmark_ticker]

    # variableåˆ—ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¦è¾æ›¸åŒ–
    df_dict = {}
    for col in return_cols:
        active_return_col = col.replace("Return", "Active_Return")

        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        df_slice = df_opt[df_opt["variable"] == active_return_col].reset_index(
            drop=True
        )

        if not df_slice.empty:
            # è‡ªèº«ã®é‡è¤‡ã‚’é™¤å»
            df_slice = df_slice.drop_duplicates(
                subset=["date", "P_SYMBOL", "variable"], ignore_index=True
            )
            df_dict[active_return_col] = df_slice

    prep_time = time.time() - prep_start

    if verbose:
        logger.info(
            "Preprocessing completed",
            elapsed_sec=round(prep_time, 2),
            tables=len(df_dict),
        )

    # --------------------------------------------------------------------------
    # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ›¸ãè¾¼ã¿(SQLiteæœ€é©åŒ–)
    # --------------------------------------------------------------------------
    results = {
        "success": [],
        "failed": [],
        "total_rows": 0,
        "prep_time": prep_time,
        "save_time": 0,
        "total_time": 0,
    }

    save_start = time.time()

    # å˜ä¸€ã®æ¥ç¶šã‚’ä½¿ç”¨
    with sqlite3.connect(db_path, timeout=30.0) as conn:
        cursor = conn.cursor()

        # ğŸ’¡ ã€æœ€é©åŒ–Aã€‘SQLiteã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨­å®š
        conn.execute("PRAGMA journal_mode=WAL")
        conn.execute("PRAGMA synchronous=NORMAL")

        # é€²æ—ãƒãƒ¼
        iterator = (
            tqdm(df_dict.items(), desc="ğŸ’¾ ä¿å­˜ä¸­") if verbose else df_dict.items()
        )

        for table_name, df in iterator:
            if df.empty:
                continue

            try:
                # ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ(å­˜åœ¨ã—ãªã„å ´åˆ)
                cursor.execute(
                    f"""
                    CREATE TABLE IF NOT EXISTS "{table_name}" (
                        date TEXT,
                        P_SYMBOL TEXT,
                        variable TEXT,
                        value REAL,
                        PRIMARY KEY (date, P_SYMBOL, variable)
                    )
                    """
                )

                # ğŸ’¡ ã€æœ€é©åŒ–Bã€‘executemanyã¨INSERT OR IGNOREã‚’ä½¿ç”¨

                # 1. æŒ¿å…¥ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¿ãƒ—ãƒ«ãƒªã‚¹ãƒˆã«å¤‰æ›
                # åˆ—ã®é †åºã¯SQLã‚¹ãƒ†ãƒ¼ãƒˆãƒ¡ãƒ³ãƒˆã®é †åºã¨ä¸€è‡´ã•ã›ã‚‹
                data_to_insert = df[
                    ["date", "P_SYMBOL", "variable", "value"]
                ].values.tolist()

                # 2. SQLã‚¹ãƒ†ãƒ¼ãƒˆãƒ¡ãƒ³ãƒˆã‚’æº–å‚™(é‡è¤‡è¡Œã¯ç„¡è¦–)
                sql_insert = f"""
                    INSERT OR IGNORE INTO "{table_name}" (date, P_SYMBOL, variable, value)
                    VALUES (?, ?, ?, ?)
                """

                # 3. executemanyã§é«˜é€Ÿã«ãƒãƒƒãƒæŒ¿å…¥ã‚’å®Ÿè¡Œ
                # executemanyã¯ã€ãƒ‡ãƒ¼ã‚¿ãƒªã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã¦å‡¦ç†ã™ã‚‹(batch_sizeã¯äº‹å®Ÿä¸Šç„¡è¦–ã•ã‚Œã‚‹)
                cursor.executemany(sql_insert, data_to_insert)

                # 4. æŒ¿å…¥ã•ã‚ŒãŸè¡Œæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ (rowcountã¯executemanyã§ã¯ä¿¡é ¼ã§ããªã„ãŸã‚ã€åˆ¥ã®æ–¹æ³•ã§å–å¾—ãŒå¿…è¦ã ãŒã€ã“ã“ã§ã¯ç°¡ç•¥åŒ–)
                # ç°¡ç•¥åŒ–ã®ãŸã‚ã€æŒ¿å…¥å¯¾è±¡ã ã£ãŸå…¨è¡Œæ•°ã‚’ä½¿ç”¨
                row_count = len(data_to_insert)

                conn.commit()

                results["success"].append(table_name)
                results["total_rows"] += (
                    row_count  # å®Ÿéš›ã«ã¯é‡è¤‡ç„¡è¦–ã§æŒ¿å…¥ã•ã‚Œãªã‹ã£ãŸè¡Œã‚‚å«ã¾ã‚Œã‚‹ãŒã€å…ƒã®ã‚³ãƒ¼ãƒ‰ã«åˆã‚ã›ã‚‹
                )

                if verbose:
                    logger.debug(
                        "Table insert attempted", table=table_name, rows=row_count
                    )

            except Exception as e:
                results["failed"].append({"table": table_name, "error": str(e)})
                logger.error(
                    "Table save failed", table=table_name, error=str(e), exc_info=True
                )

                # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯
                with contextlib.suppress(BaseException):
                    conn.rollback()

    results["save_time"] = time.time() - save_start
    results["total_time"] = time.time() - start_time

    # --------------------------------------------------------------------------
    # ã‚¹ãƒ†ãƒƒãƒ—3: çµ±è¨ˆæƒ…å ±è¡¨ç¤º
    # --------------------------------------------------------------------------
    if verbose:
        success_rate = (len(results["success"]) / len(df_dict) * 100) if df_dict else 0
        throughput = (
            results["total_rows"] / results["total_time"]
            if results["total_time"] > 0
            else 0
        )
        logger.info(
            "Batch save completed",
            success_count=len(results["success"]),
            total_tables=len(df_dict),
            failed_count=len(results["failed"]),
            total_attempted_rows=results["total_rows"],
            prep_time_sec=round(results["prep_time"], 2),
            save_time_sec=round(results["save_time"], 2),
            total_time_sec=round(results["total_time"], 2),
            throughput_per_sec=round(throughput),
            success_rate_pct=round(success_rate, 1),
        )

        if results["failed"]:
            for failed in results["failed"]:
                logger.warning(
                    "Failed table", table=failed["table"], error=failed["error"]
                )

    return results


# ============================================================================================
def store_to_database_batch(
    df_dict: dict[str, pd.DataFrame],
    db_path: Path,
    unique_cols: list[str] | None = None,
    batch_size: int = 10000,
    max_workers: int | None = 1,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’1ã«å¤‰æ›´(ãƒ­ãƒƒã‚¯å›é¿)
    verbose: bool = True,
) -> dict[str, Any]:
    """
    è¤‡æ•°ã®DataFrameã‚’ãƒãƒƒãƒä¿å­˜ã§ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ›¸ãè¾¼ã‚€

    âš ï¸ æ³¨æ„: max_workersã‚’1ã«ã™ã‚‹ã“ã¨ã§ãƒ­ãƒƒã‚¯å•é¡Œã‚’å›é¿ã—ã¾ã™ã€‚
    ä¸¦åˆ—å‡¦ç†ãŒå¿…è¦ãªå ´åˆã¯ã€äº‹å‰ã«WALãƒ¢ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹åŒ–ã—ã¦ãã ã•ã„ã€‚

    :param df_dict: {table_name: DataFrame}ã®è¾æ›¸
    :param db_path: SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    :param unique_cols: ä¸€æ„æ€§ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹ã‚«ãƒ©ãƒ ã®ãƒªã‚¹ãƒˆ
    :param batch_size: ä¸€åº¦ã«æŒ¿å…¥ã™ã‚‹è¡Œæ•°
    :param max_workers: ä¸¦åˆ—å®Ÿè¡Œã™ã‚‹æœ€å¤§ã‚¹ãƒ¬ãƒƒãƒ‰æ•°(1æ¨å¥¨)
    :param verbose: é€²æ—è¡¨ç¤ºãƒ•ãƒ©ã‚°
    :return: å‡¦ç†çµæœã®çµ±è¨ˆæƒ…å ±
    """
    if unique_cols is None:
        unique_cols = ["date", "P_SYMBOL", "variable"]

    if not df_dict:
        if verbose:
            logger.warning("No data to save")
        return {"success": 0, "failed": 0, "total_rows": 0, "processing_time": 0}

    # ãƒ†ãƒ¼ãƒ–ãƒ«åã¨ã‚«ãƒ©ãƒ åã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆSQLã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³å¯¾ç­–ï¼‰
    for table_name in df_dict:
        _validate_sql_identifier(table_name)
    for col in unique_cols:
        _validate_sql_identifier(col)

    if max_workers is not None and max_workers > 1 and verbose:
        logger.warning(
            "max_workers > 1 risks database locks",
            max_workers=max_workers,
        )

    if verbose:
        logger.info(
            "Batch save mode started",
            tables=len(df_dict),
            batch_size=batch_size,
            max_workers=max_workers,
            database=Path(db_path).name,
        )

    import time

    start_time = time.time()

    # --------------------------------------------------------------------------
    # ã‚¹ãƒ†ãƒƒãƒ—1: äº‹å‰ã«ãƒ‡ãƒ¼ã‚¿å‹ã‚’ä¸€æ‹¬å¤‰æ›(å…¨DataFrameã«å¯¾ã—ã¦)
    # --------------------------------------------------------------------------
    if verbose:
        logger.debug("Data type conversion started")

    optimized_dict = {}
    prep_start = time.time()

    for table_name, df in df_dict.items():
        df_opt = df.copy()

        # å¿…é ˆã‚«ãƒ©ãƒ ã®ãƒã‚§ãƒƒã‚¯
        if not all(col in df_opt.columns for col in unique_cols):
            if verbose:
                logger.warning(
                    "Skipping table, missing required columns", table=table_name
                )
            continue

        # ãƒ‡ãƒ¼ã‚¿å‹å¤‰æ›
        if "date" in df_opt.columns:
            df_opt["date"] = pd.to_datetime(df_opt["date"])
        if "value" in df_opt.columns:
            df_opt["value"] = df_opt["value"].astype(float)

        # é‡è¤‡é™¤å»
        df_opt = df_opt.drop_duplicates(subset=unique_cols, ignore_index=True)

        if not df_opt.empty:
            optimized_dict[table_name] = df_opt

    prep_time = time.time() - prep_start

    if verbose:
        logger.info(
            "Preprocessing completed",
            elapsed_sec=round(prep_time, 2),
            tables=len(optimized_dict),
        )

    # --------------------------------------------------------------------------
    # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ¯ãƒ¼ã‚«ãƒ¼é–¢æ•°
    # --------------------------------------------------------------------------
    def _batch_save_worker(args: tuple) -> tuple:
        """ãƒãƒƒãƒä¿å­˜ãƒ¯ãƒ¼ã‚«ãƒ¼"""
        table_name, df = args

        try:
            with sqlite3.connect(db_path, timeout=30.0) as conn:
                cursor = conn.cursor()

                # ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
                # table_name, unique_cols ã¯é–¢æ•°å…ˆé ­ã§ _validate_sql_identifier() æ¤œè¨¼æ¸ˆã¿
                cursor.execute(
                    f"""
                    CREATE TABLE IF NOT EXISTS "{table_name}" (
                        date TEXT,
                        P_SYMBOL TEXT,
                        variable TEXT,
                        value REAL,
                        PRIMARY KEY ({",".join(unique_cols)})
                    )
                    """  # nosec B608
                )

                # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                try:
                    select_cols = ", ".join(unique_cols)
                    # table_name, unique_cols ã¯é–¢æ•°å…ˆé ­ã§æ¤œè¨¼æ¸ˆã¿
                    query = f'SELECT {select_cols} FROM "{table_name}"'  # nosec B608
                    existing_df = pd.read_sql(query, conn)

                    if "date" in unique_cols and "date" in existing_df.columns:
                        existing_df["date"] = pd.to_datetime(existing_df["date"])

                    merged_df = pd.merge(
                        df,
                        existing_df,
                        on=unique_cols,
                        how="left",
                        indicator=True,
                    )

                    df_to_add = merged_df[merged_df["_merge"] == "left_only"].drop(
                        columns=["_merge"]
                    )

                    if df_to_add.empty:
                        return (table_name, True, 0, "é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã®ã¿")

                except pd.io.sql.DatabaseError:  # type: ignore
                    df_to_add = df

                row_count = len(df_to_add)

                # ãƒãƒƒãƒINSERT
                df_to_add.to_sql(
                    table_name,
                    conn,
                    if_exists="append",
                    index=False,
                    chunksize=batch_size,
                    method="multi",
                )

                conn.commit()

                return (table_name, True, row_count, None)

        except Exception as e:
            return (table_name, False, 0, str(e))

    # --------------------------------------------------------------------------
    # ã‚¹ãƒ†ãƒƒãƒ—3: å®Ÿè¡Œ
    # --------------------------------------------------------------------------
    results = {
        "success": [],
        "failed": [],
        "total_rows": 0,
        "prep_time": prep_time,
        "save_time": 0,
        "total_time": 0,
    }

    save_start = time.time()
    args_list = list(optimized_dict.items())

    if max_workers == 1:
        # ç›´åˆ—å‡¦ç†
        iterator = tqdm(args_list, desc="ğŸ’¾ ä¿å­˜ä¸­") if verbose else args_list

        for args in iterator:
            table_name, success, row_count, error_msg = _batch_save_worker(args)

            if success:
                results["success"].append(table_name)
                results["total_rows"] += row_count

                if verbose and row_count > 0:
                    logger.debug("Table saved", table=table_name, rows=row_count)
                elif verbose:
                    logger.debug("Table skipped", table=table_name, reason=error_msg)
            else:
                results["failed"].append({"table": table_name, "error": error_msg})
                logger.error("Table save failed", table=table_name, error=error_msg)

    else:
        # ä¸¦åˆ—å‡¦ç†(WALãƒ¢ãƒ¼ãƒ‰æ¨å¥¨)
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {
                executor.submit(_batch_save_worker, args): args[0] for args in args_list
            }

            if verbose:
                futures_iterator = tqdm(
                    as_completed(futures), total=len(futures), desc="Saving"
                )
            else:
                futures_iterator = as_completed(futures)

            for future in futures_iterator:
                table_name = futures[future]

                try:
                    table_name_result, success, row_count, error_msg = future.result()

                    if success:
                        results["success"].append(table_name_result)
                        results["total_rows"] += row_count

                        if verbose and row_count > 0:
                            logger.debug(
                                "Table saved", table=table_name_result, rows=row_count
                            )
                        elif verbose:
                            logger.debug(
                                "Table skipped",
                                table=table_name_result,
                                reason=error_msg,
                            )
                    else:
                        results["failed"].append(
                            {"table": table_name_result, "error": error_msg}
                        )
                        logger.error(
                            "Table save failed",
                            table=table_name_result,
                            error=error_msg,
                        )

                except Exception as e:
                    results["failed"].append({"table": table_name, "error": str(e)})
                    logger.error(
                        "Table save failed",
                        table=table_name,
                        error=str(e),
                        exc_info=True,
                    )

    results["save_time"] = time.time() - save_start
    results["total_time"] = time.time() - start_time

    # --------------------------------------------------------------------------
    # çµ±è¨ˆæƒ…å ±
    # --------------------------------------------------------------------------
    if verbose:
        throughput = (
            results["total_rows"] / results["total_time"]
            if results["total_time"] > 0
            else 0
        )
        logger.info(
            "Batch save completed",
            success_count=len(results["success"]),
            total_tables=len(df_dict),
            failed_count=len(results["failed"]),
            total_rows=results["total_rows"],
            prep_time_sec=round(results["prep_time"], 2),
            save_time_sec=round(results["save_time"], 2),
            total_time_sec=round(results["total_time"], 2),
            throughput_per_sec=round(throughput),
        )

        if results["failed"]:
            for failed in results["failed"]:
                logger.warning(
                    "Failed table", table=failed["table"], error=failed["error"]
                )

    return results


# ============================================================================================
def store_active_returns_batch(
    df_active_returns: pd.DataFrame,
    return_cols: list[str],
    db_path: Path,
    benchmark_ticker: str,
    batch_size: int = 10000,
    max_workers: int | None = None,
    verbose: bool = True,
) -> dict[str, Any]:
    """
    ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒªã‚¿ãƒ¼ãƒ³ã‚’ãƒãƒƒãƒä¿å­˜(æœ€é©åŒ–ç‰ˆ)

    å…ƒã®ã‚³ãƒ¼ãƒ‰ã‚’ç½®ãæ›ãˆã‚‹é–¢æ•°ã€‚äº‹å‰ã«variableåˆ—ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¦ã‹ã‚‰
    ãƒãƒƒãƒä¿å­˜ã™ã‚‹ã“ã¨ã§å¤§å¹…ãªé«˜é€ŸåŒ–ã‚’å®Ÿç¾ã€‚

    :param df_active_returns: ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒªã‚¿ãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿(Longå½¢å¼)
    :param return_cols: å‡¦ç†å¯¾è±¡ã®ãƒªã‚¿ãƒ¼ãƒ³åˆ—åãƒªã‚¹ãƒˆ
    :param db_path: SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    :param benchmark_ticker: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚£ãƒƒã‚«ãƒ¼(é™¤å¤–ç”¨)
    :param batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
    :param max_workers: ä¸¦åˆ—åº¦
    :param verbose: é€²æ—è¡¨ç¤º
    :return: å‡¦ç†çµæœ
    """
    if verbose:
        logger.info(
            "Active returns optimized batch save started",
            columns=len(return_cols),
            rows=len(df_active_returns),
        )

    # ãƒ‡ãƒ¼ã‚¿å‹å¤‰æ›ã¨ãƒªãƒãƒ¼ãƒ 
    df_opt = df_active_returns.copy()
    df_opt["value"] = df_opt["value"].astype(float)
    df_opt["date"] = pd.to_datetime(df_opt["date"])
    df_opt = df_opt.rename(columns={"symbol": "P_SYMBOL"})

    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é™¤å¤–
    df_opt = df_opt[df_opt["P_SYMBOL"] != benchmark_ticker]

    if verbose:
        logger.debug("Pre-splitting data")

    # variableåˆ—ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¦è¾æ›¸åŒ–
    df_dict = {}
    for col in return_cols:
        active_return_col = col.replace("Return", "Active_Return")

        # queryã®ä»£ã‚ã‚Šã«é«˜é€Ÿãªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        df_slice = df_opt[df_opt["variable"] == active_return_col].reset_index(
            drop=True
        )

        if not df_slice.empty:
            df_dict[active_return_col] = df_slice

    if verbose:
        logger.info("Split completed", tables=len(df_dict))

    # ãƒãƒƒãƒä¿å­˜å®Ÿè¡Œ
    results = store_to_database_batch(
        df_dict=df_dict,
        db_path=db_path,
        unique_cols=["date", "P_SYMBOL", "variable"],
        batch_size=batch_size,
        max_workers=max_workers,
        verbose=verbose,
    )

    return results


# ============================================================================================
def ensure_unique_constraint(conn: sqlite3.Connection, table_name: str):
    """
    ãƒ†ãƒ¼ãƒ–ãƒ«ã«UNIQUEåˆ¶ç´„ãŒã‚ã‚‹ã‹ç¢ºèªã—ã€ãªã‘ã‚Œã°å†ä½œæˆ
    """
    cursor = conn.cursor()

    # ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã‚’å–å¾—
    cursor.execute(f"PRAGMA table_info('{table_name}')")
    columns_info = cursor.fetchall()

    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æƒ…å ±ã‚’å–å¾—
    cursor.execute(f"PRAGMA index_list('{table_name}')")
    indexes = cursor.fetchall()

    # PRIMARY KEYã¾ãŸã¯UNIQUEã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒ(date, P_SYMBOL, variable)ã«ã‚ã‚‹ã‹ç¢ºèª
    has_constraint = False
    for index in indexes:
        index_name = index[1]
        cursor.execute(f"PRAGMA index_info('{index_name}')")
        index_columns = [col[2] for col in cursor.fetchall()]

        if set(index_columns) == {"date", "P_SYMBOL", "variable"}:
            has_constraint = True
            break

    if not has_constraint:
        logger.warning("Table missing UNIQUE constraint", table=table_name)
        return False

    return True


# ============================================================================================
def upsert_financial_data(
    df: pd.DataFrame,
    conn: sqlite3.Connection,
    table_name: str,
    method: str = "auto",  # "auto", "upsert", "delete_insert"
):
    """
    è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°

    Parameters
    ----------
    df : pd.DataFrame
        æ›´æ–°ãƒ‡ãƒ¼ã‚¿ (columns: date, P_SYMBOL, variable, value)
    conn : sqlite3.Connection
        ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
    table_name : str
        ãƒ†ãƒ¼ãƒ–ãƒ«å
    method : str
        æ›´æ–°æ–¹æ³• ("auto", "upsert", "delete_insert")
        - "auto": SQLiteãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«å¿œã˜ã¦è‡ªå‹•é¸æŠ
        - "upsert": UPSERTæ§‹æ–‡ã‚’ä½¿ç”¨(SQLite 3.24.0ä»¥é™)
        - "delete_insert": DELETE + INSERTæ–¹å¼
    """

    if df.empty:
        logger.warning("Empty update data", table=table_name)
        return 0

    # ãƒ†ãƒ¼ãƒ–ãƒ«åã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆSQLã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³å¯¾ç­–ï¼‰
    _validate_sql_identifier(table_name)

    # SQLiteãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
    sqlite_version = tuple(map(int, sqlite3.sqlite_version.split(".")))
    supports_upsert = sqlite_version >= (3, 24, 0)

    # methodãŒ"auto"ã®å ´åˆã€ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«å¿œã˜ã¦è‡ªå‹•é¸æŠ
    if method == "auto":
        method = "upsert" if supports_upsert else "delete_insert"
        logger.info(
            "Auto-selected upsert method",
            sqlite_version=sqlite3.sqlite_version,
            method=method,
        )

    # æ—¥ä»˜ã‚’æ–‡å­—åˆ—ã«å¤‰æ›
    df_copy = df.copy()
    df_copy["date"] = pd.to_datetime(df_copy["date"]).dt.strftime("%Y-%m-%d")

    # NaN ã‚’ None ã«å¤‰æ›(SQLiteã®NULL)
    df_copy = df_copy.where(pd.notnull(df_copy), None)

    # ä¸€æ™‚ãƒ†ãƒ¼ãƒ–ãƒ«å
    temp_table = f"{table_name}_temp_{np.random.randint(1000, 9999)}"

    try:
        # ä¸€æ™‚ãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¿å­˜
        df_copy.to_sql(temp_table, conn, if_exists="replace", index=False)

        cursor = conn.cursor()

        if method == "upsert":
            if not supports_upsert:
                logger.warning(
                    "UPSERT not supported, falling back to delete_insert",
                    sqlite_version=sqlite3.sqlite_version,
                )
                method = "delete_insert"
            elif not ensure_unique_constraint(conn, table_name):
                logger.warning(
                    "UPSERT unavailable, falling back to delete_insert",
                    table=table_name,
                )
                method = "delete_insert"

        if method == "upsert":
            # UPSERTæ–¹å¼(SQLite 3.24.0ä»¥é™)
            # table_name, temp_table ã¯ _validate_sql_identifier() ã§æ¤œè¨¼æ¸ˆã¿
            upsert_query = f"""
                INSERT INTO "{table_name}" (date, P_SYMBOL, variable, value)
                SELECT date, P_SYMBOL, variable, value
                FROM "{temp_table}"
                ON CONFLICT(date, P_SYMBOL, variable)
                DO UPDATE SET value = excluded.value
            """  # nosec B608 - table_name ã¯æ¤œè¨¼æ¸ˆã¿
            cursor.execute(upsert_query)
            rows_affected = cursor.rowcount

        elif method == "delete_insert":
            # DELETE + INSERTæ–¹å¼(å¤ã„SQLiteã§ã‚‚å‹•ä½œ)
            # table_name, temp_table ã¯ _validate_sql_identifier() ã§æ¤œè¨¼æ¸ˆã¿
            # 1. è©²å½“è¡Œã‚’å‰Šé™¤
            delete_query = f"""
                DELETE FROM "{table_name}"
                WHERE (date, P_SYMBOL, variable) IN (
                    SELECT date, P_SYMBOL, variable
                    FROM "{temp_table}"
                )
            """  # nosec B608 - table_name ã¯æ¤œè¨¼æ¸ˆã¿
            cursor.execute(delete_query)
            deleted = cursor.rowcount

            # 2. æ–°è¦ãƒ‡ãƒ¼ã‚¿ã‚’æŒ¿å…¥
            insert_query = f"""
                INSERT INTO "{table_name}" (date, P_SYMBOL, variable, value)
                SELECT date, P_SYMBOL, variable, value
                FROM "{temp_table}"
            """  # nosec B608 - table_name ã¯æ¤œè¨¼æ¸ˆã¿
            cursor.execute(insert_query)
            inserted = cursor.rowcount

            rows_affected = inserted

            logger.info(
                "Delete-insert completed",
                table=table_name,
                deleted=deleted,
                inserted=inserted,
            )

        else:
            raise ValueError(f"ä¸æ­£ãªmethod: {method}")

        # ä¸€æ™‚ãƒ†ãƒ¼ãƒ–ãƒ«å‰Šé™¤
        # nosec B608 - temp_table ã¯ table_name ã‹ã‚‰ç”Ÿæˆã•ã‚ŒãŸå®‰å…¨ãªè­˜åˆ¥å­
        cursor.execute(f'DROP TABLE IF EXISTS "{temp_table}"')

        conn.commit()

        logger.info("Upsert completed", table=table_name, rows_affected=rows_affected)

        return rows_affected

    except Exception as e:
        logger.error("Upsert failed", table=table_name, error=str(e), exc_info=True)
        conn.rollback()
        # ä¸€æ™‚ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        with contextlib.suppress(BaseException):
            # nosec B608 - temp_table ã¯ table_name ã‹ã‚‰ç”Ÿæˆã•ã‚ŒãŸå®‰å…¨ãªè­˜åˆ¥å­
            cursor.execute(f'DROP TABLE IF EXISTS "{temp_table}"')  # nosec B608
        raise


# ============================================================================================
def load_index_constituents(
    factset_index_db_path: Path, UNIVERSE_CODE: str
) -> pd.DataFrame:
    """
    BPMã¨Factsetã®ã‚³ãƒ¼ãƒ‰ã‚’ãƒãƒ¼ã‚¸ã—ãŸã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹æˆéŠ˜æŸ„ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰
    å¿…è¦ãªã‚«ãƒ©ãƒ ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹é–¢æ•°
    """
    # ãƒ†ãƒ¼ãƒ–ãƒ«åã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆSQLã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³å¯¾ç­–ï¼‰
    _validate_sql_identifier(UNIVERSE_CODE)

    with sqlite3.connect(factset_index_db_path) as conn:
        # UNIVERSE_CODE ã¯ _validate_sql_identifier() ã§æ¤œè¨¼æ¸ˆã¿
        query = f"""
            SELECT
                `date`, `P_SYMBOL`, `SEDOL`, `Asset ID`, `FG_COMPANY_NAME`,
                `GICS Sector`, `GICS Industry Group`,
                `Weight (%)`
            FROM
                {UNIVERSE_CODE}
            """  # nosec B608
        df_weight = pd.read_sql(
            query,
            con=conn,
            parse_dates=["date"],
        )

    return df_weight


# ============================================================================================
def load_financial_data(
    financials_db_path: Path, factor_list: list[str]
) -> pd.DataFrame:
    """
    Factsetã®è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã¨ãƒªã‚¿ãƒ¼ãƒ³ã‚’æ ¼ç´ã—ãŸãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ãƒ­ãƒ¼ãƒ‰ã™ã‚‹é–¢æ•°
    factor_listã§ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã‚’æŒ‡å®šã™ã‚‹
    ä¾‹:
        factor_list = [
            "FF_ROIC",
            "FF_ROIC_PctRank",
            "FF_ROIC_Rank",
            "FF_ROE",
            "FF_ROE_PctRank",
            "FF_ROE_Rank",
            "Active_Return_1M_annlzd",
            "Active_Return_3M_annlzd",
            "Active_Return_6M_annlzd",
            "Active_Return_12M_annlzd",
            "Active_Return_3Y_annlzd",
            "Active_Return_5Y_annlzd",
        ]
    """
    # ãƒ†ãƒ¼ãƒ–ãƒ«åã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆSQLã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³å¯¾ç­–ï¼‰
    for factor in factor_list:
        _validate_sql_identifier(factor)

    # join()ã‚’ä½¿ç”¨ã—ã¦ã‚¯ã‚¨ãƒªã‚’UNION ALLã™ã‚‹
    # factor_list ã®å„è¦ç´ ã¯ _validate_sql_identifier() ã§æ¤œè¨¼æ¸ˆã¿
    query = """
        SELECT `date`, `P_SYMBOL`, `variable`, `value` FROM {}
    """.format(  # nosec B608
        "\n    UNION ALL\n    SELECT `date`, `P_SYMBOL`, `variable`, `value` FROM ".join(
            factor_list
        )
    )

    with sqlite3.connect(financials_db_path) as conn:
        df_factor = (
            pd.read_sql(query, parse_dates=["date"], con=conn)
            .assign(
                # dateåˆ—ã¯BPM+Factsetã‚³ãƒ¼ãƒ‰ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨ç•°ãªã‚‹
                # BPM+Factsetã¯æœˆæœ«æ—¥
                # ä¸€æ–¹ã€Factsetã®è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã¯æœˆæœ«æœ€çµ‚å–¶æ¥­æ—¥
                # ã‚ˆã£ã¦ã€ã“ã“ã§ã¯å¼·åˆ¶çš„ã«æœˆæœ«æ—¥ã«æƒãˆã‚‹
                date=lambda x: pd.to_datetime(x["date"]) + pd.offsets.MonthEnd(0)
            )
            .drop_duplicates(ignore_index=True)
        )
        # long formatã«å¤‰æ›
        df_factor = pd.pivot(
            df_factor, index=["date", "P_SYMBOL"], columns="variable", values="value"
        ).reset_index()

    return df_factor


# ============================================================================================
def merge_idx_constituents_and_financials(
    df_weight: pd.DataFrame, df_factor: pd.DataFrame
) -> pd.DataFrame:
    """
    load_index_constituents()ã¨load_financial_data()ã®çµæœã‚’ãƒãƒ¼ã‚¸ã™ã‚‹é–¢æ•°
    """
    df_merged = pd.merge(
        df_weight, df_factor, on=["date", "P_SYMBOL"], how="outer"
    ).dropna(subset=["Weight (%)"], ignore_index=True)

    return df_merged


# ============================================================================================
@dataclass
class FactorJobArgs:
    """ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼è¨ˆç®—ã‚¸ãƒ§ãƒ–ã®ãŸã‚ã®å¼•æ•°ã‚’æ ¼ç´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹ã€‚

    Attributes:
        factor (str): ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼å(ä¾‹: "FF_ROIC")ã€‚
        db_path (Union[str, Path]): ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã€‚
        df_weight (pd.DataFrame): ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹æˆéŠ˜æŸ„ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ (Weightåˆ—ãªã©ã‚’å«ã‚€)ã€‚
        sector_neutral_mode (bool): ã‚»ã‚¯ã‚¿ãƒ¼ä¸­ç«‹åŒ–ã‚’è¡Œã†ã‹ã©ã†ã‹ã®ãƒ•ãƒ©ã‚°ã€‚
        inversed (bool): ãƒ©ãƒ³ã‚¯ã‚’é€†è»¢ã•ã›ã‚‹ã‹ã©ã†ã‹ã®ãƒ•ãƒ©ã‚°(ä¾‹: ä½ã„æ–¹ãŒè‰¯ã„æŒ‡æ¨™ã®å ´åˆ)ã€‚
        period (Optional[str]): æœŸé–“æŒ‡å®šã€‚æŒ‡å®šãŒãªã„å ´åˆã¯Noneã€‚
            æƒ³å®šã•ã‚Œã‚‹å€¤: ["QoQ", "YoY", "CAGR_3Y", "CAGR_5Y"] ãªã©ã€‚
            æŒ‡å®šã•ã‚ŒãŸå ´åˆã€ãƒ†ãƒ¼ãƒ–ãƒ«åã¯ `{factor}_{period}` ã¨ãªã‚Šã¾ã™ã€‚
        winsorize (bool): add_factor_rank_colã¨add_factor_pct_rank_colsé–¢æ•°ã§
            winsorizeã™ã‚‹å ´åˆã¯True, ã—ãªã„å ´åˆã¯False
        winsorize_limits (tule): winsorizeã®ä¸¡ç«¯ä½•%ã§åˆ‡ã‚‹ã‹
    """

    factor: str
    db_path: str | Path
    df_weight: pd.DataFrame
    sector_neutral_mode: bool
    inversed: bool
    period: str | None = None
    winsorize: bool = True
    winsorize_limits: tuple = (0.01, 0.01)


# --------------------------------------------------------------------------------------------
def process_ranking_factor_worker(
    job_args: FactorJobArgs,
) -> list[tuple[str, pd.DataFrame]]:
    """å˜ä¸€ã®ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ»æœŸé–“ã«å¯¾ã—ã¦ãƒ©ãƒ³ã‚¯ç­‰ã®æŒ‡æ¨™è¨ˆç®—ã‚’è¡Œã†ãƒ¯ãƒ¼ã‚«ãƒ¼é–¢æ•°ã€‚

    DataClassçµŒç”±ã§å¼•æ•°ã‚’å—ã‘å–ã‚‹ãŸã‚ã€å¼•æ•°ã®é †åºä¾å­˜æ€§ãŒã‚ã‚Šã¾ã›ã‚“ã€‚
    periodãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ `{factor}_{period}` ã‚’ã€
    æŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯ `{factor}` ã‚’ãƒ†ãƒ¼ãƒ–ãƒ«åãŠã‚ˆã³ã‚«ãƒ©ãƒ åã¨ã—ã¦ä½¿ç”¨ã—ã¾ã™ã€‚

    Args:
        job_args (FactorJobArgs): è¨ˆç®—ã«å¿…è¦ãªå…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ ¼ç´ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹ã€‚

    Returns:
        List[Tuple[str, pd.DataFrame]]: è¨ˆç®—çµæœã®ãƒªã‚¹ãƒˆã€‚
            å„è¦ç´ ã¯ (ãƒ†ãƒ¼ãƒ–ãƒ«å/ã‚«ãƒ©ãƒ å, è¨ˆç®—æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ) ã®ã‚¿ãƒ—ãƒ«ã€‚
            ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã‚„è©²å½“ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯ç©ºãƒªã‚¹ãƒˆã‚’è¿”ã—ã¾ã™ã€‚
    """
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå(ãƒ†ãƒ¼ãƒ–ãƒ«åãƒ»ã‚«ãƒ©ãƒ å)ã®æ±ºå®š
    # ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼åã¨ãƒ”ãƒªã‚ªãƒ‰ã‚’ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆSQLã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³å¯¾ç­–ï¼‰
    _validate_sql_identifier(job_args.factor)
    if job_args.period:
        _validate_sql_identifier(job_args.period)
        target_factor_name = f"{job_args.factor}_{job_args.period}"
    else:
        target_factor_name = job_args.factor

    results = []

    try:
        # 1. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹èª­ã¿è¾¼ã¿
        # target_factor_name ã¯ _validate_sql_identifier() ã§æ¤œè¨¼æ¸ˆã¿
        query = f"SELECT `date`, `P_SYMBOL`, `value` FROM '{target_factor_name}'"  # nosec B608

        with sqlite3.connect(job_args.db_path) as conn:
            df = pd.read_sql(query, con=conn, parse_dates=["date"])

        # æ—¥ä»˜èª¿æ•´(æœˆæœ«æƒãˆ)ã¨ãƒªãƒãƒ¼ãƒ 
        df["date"] = pd.to_datetime(df["date"]) + pd.tseries.offsets.MonthEnd(0)
        df = df.sort_values("date", ignore_index=True).rename(
            columns={"value": target_factor_name}
        )

        # 2. ãƒãƒ¼ã‚¸å‡¦ç†
        # æ§‹æˆéŠ˜æŸ„æƒ…å ±ã¨çµåˆã—ã¦ãƒ¦ãƒ‹ãƒãƒ¼ã‚¹ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        df_merged = pd.merge(
            job_args.df_weight, df, on=["date", "P_SYMBOL"], how="outer"
        )

        # ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°: é‡è¤‡æ’é™¤ã¨å¿…é ˆã‚«ãƒ©ãƒ ã®æ¬ æé™¤å»
        df_merged = df_merged.drop_duplicates(subset=["date", "P_SYMBOL"]).dropna(
            subset=["Weight (%)", target_factor_name],
            how="any",
            axis=0,
            ignore_index=True,
        )

        if df_merged.empty:
            return results

        # 3. æŒ‡æ¨™è¨ˆç®— (Rank, PctRank, ZScore)
        # -------------------------------------------------------------
        # å†…éƒ¨é–¢æ•°ã§å‡¦ç†ã‚’å…±é€šåŒ–
        def _add_metric_to_results(
            metric_type: str,
            calculation_func: Callable[..., Any],
        ):
            """è¨ˆç®—ã‚’å®Ÿè¡Œã—ã€çµæœãƒªã‚¹ãƒˆã«è¿½åŠ ã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã€‚"""
            # ğŸ”§ ä¿®æ­£ç®‡æ‰€: é–¢æ•°ã®ã‚·ã‚°ãƒãƒãƒ£ã‚’æ¤œæŸ»
            import inspect

            func_params = inspect.signature(calculation_func).parameters

            # winsorizeå¼•æ•°ã‚’æŒã¤ã‹ãƒã‚§ãƒƒã‚¯
            has_winsorize = "winsorize" in func_params
            has_winsorize_limits = "winsorize_limits" in func_params

            # åŸºæœ¬å¼•æ•°
            kwargs = {
                "df": df_merged,
                "factor_name": target_factor_name,
                "sector_neutral_mode": job_args.sector_neutral_mode,
                "inversed": job_args.inversed,
            }

            # winsorizeå¼•æ•°ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿è¿½åŠ 
            if has_winsorize:
                kwargs["winsorize"] = job_args.winsorize
            if has_winsorize_limits:
                kwargs["winsorize_limits"] = job_args.winsorize_limits

            # roic_utilsã®è¨ˆç®—é–¢æ•°ã‚’å‘¼ã³å‡ºã—
            df_res = calculation_func(**kwargs)

            # ã‚«ãƒ©ãƒ åã®æ§‹ç¯‰ (ä¾‹: Factor_Inv_Rank ã¾ãŸã¯ Factor_Rank)
            prefix = (
                f"{target_factor_name}_Inv"
                if job_args.inversed
                else f"{target_factor_name}"
            )
            col_name = f"{prefix}_{metric_type}"
            # ã‚»ã‚¯ã‚¿ãƒ¼ä¸­ç«‹ã®ãƒ©ãƒ™ãƒ«
            if job_args.sector_neutral_mode:
                col_name = f"{col_name}_Sector_Neutral"

            # ãƒ†ãƒ¼ãƒ–ãƒ«å
            table_name = col_name

            # å¿…è¦ãªåˆ—ã®ã¿æŠ½å‡ºã—ã€long formatç”¨ã®ã‚«ãƒ©ãƒ æ§‹æˆã«ã™ã‚‹
            df_res = (
                df_res[["date", "P_SYMBOL", col_name]]
                .rename(columns={col_name: "value"})
                .assign(variable=col_name)
            )
            results.append((table_name, df_res))

        # -------------------------------------------------------------

        # å„æŒ‡æ¨™ã®è¨ˆç®—å®Ÿè¡Œ
        _add_metric_to_results("Rank", roic_utils.add_factor_rank_cols)  # type: ignore[name-defined]
        _add_metric_to_results("PctRank", roic_utils.add_factor_pct_rank_cols)  # type: ignore[name-defined]
        _add_metric_to_results("ZScore", roic_utils.add_factor_zscore_cols)  # type: ignore[name-defined]

        return results

    except Exception as e:
        # ä¸¦åˆ—å‡¦ç†ä¸­ã®ã‚¨ãƒ©ãƒ¼ã¯ãƒ­ã‚°ã«å‡ºåŠ›ã—ã€ãƒ—ãƒ­ã‚»ã‚¹ã‚’è½ã¨ã•ãšã«ç©ºãƒªã‚¹ãƒˆã‚’è¿”ã™
        logger.error(
            "Factor processing failed",
            factor=target_factor_name,
            error=str(e),
            exc_info=True,
        )
        return []


# ============================================================================================
def process_rank_calculation_store_to_db(
    df_weight: pd.DataFrame,
    factor_list: list[str],
    financials_db_path: Path,
    period_list: list[str] | None = None,
    sector_neutral_mode: bool = True,
    inversed: bool = False,
    winsorize: bool = True,
    winsorize_limits: tuple = (0.01, 0.01),
    default_max_workers: int = 6,
):
    """ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã®ãƒ©ãƒ³ã‚¯è¨ˆç®—ã‚’è¡Œã„ã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã™ã‚‹é–¢æ•°ã€‚

    period_list ã®æŒ‡å®šæœ‰ç„¡ã«ã‚ˆã‚Šã€å˜ä¸€ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼è¨ˆç®—ã¨æœŸé–“ä»˜ãè¨ˆç®—ã®ä¸¡æ–¹ã«å¯¾å¿œã—ã¾ã™ã€‚

    Args:
        df_weight (pd.DataFrame): ã‚¦ã‚§ã‚¤ãƒˆæƒ…å ±ã‚’å«ã‚€æ§‹æˆéŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿ã€‚
        factor_list (List[str]): è¨ˆç®—å¯¾è±¡ã®ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼åã®ãƒªã‚¹ãƒˆã€‚
        financials_db_path (Path): ä¿å­˜å…ˆã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹ã€‚
        period_list (Optional[List[str]]): æœŸé–“ã®ãƒªã‚¹ãƒˆ (ä¾‹: ["YoY", "QoQ"])ã€‚
            æŒ‡å®šã—ãŸå ´åˆã€factor_listã¨ã®çµ„ã¿åˆã‚ã›ã§è¨ˆç®—ã‚’è¡Œã„ã¾ã™ã€‚
            Noneã®å ´åˆã€periodãªã—(å˜ä¸€ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼)ã¨ã—ã¦è¨ˆç®—ã—ã¾ã™ã€‚
        sector_neutral_mode (bool): ã‚»ã‚¯ã‚¿ãƒ¼ä¸­ç«‹åŒ–ã‚’è¡Œã†ã‹ã€‚
        inversed (bool): ãƒ©ãƒ³ã‚¯ã‚’é€†è»¢ã•ã›ã‚‹ã‹ã€‚
        default_max_workers (int): æœ€å¤§ä¸¦åˆ—ãƒ—ãƒ­ã‚»ã‚¹æ•°ã€‚
    """
    # ---------------------------------------------------------
    # 1. ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆã®ä½œæˆ (FactorJobArgsã®æº–å‚™)
    # ---------------------------------------------------------
    tasks = []

    if period_list and len(period_list) > 0:
        # ãƒ‘ã‚¿ãƒ¼ãƒ³A: æœŸé–“æŒ‡å®šã‚ã‚Š (Factor x Period ã®çµ„ã¿åˆã‚ã›ã‚’ä½œæˆ)
        mode_desc = "Multi-Period Mode"
        for factor in factor_list:
            for period in period_list:
                args = FactorJobArgs(
                    factor=factor,
                    db_path=financials_db_path,
                    df_weight=df_weight,
                    sector_neutral_mode=sector_neutral_mode,
                    inversed=inversed,
                    period=period,  # æœŸé–“ã‚’æŒ‡å®š
                    winsorize=winsorize,
                    winsorize_limits=winsorize_limits,
                )
                tasks.append(args)
    else:
        # ãƒ‘ã‚¿ãƒ¼ãƒ³B: æœŸé–“æŒ‡å®šãªã— (Single Factor Mode)
        mode_desc = "Single Factor Mode"
        for factor in factor_list:
            args = FactorJobArgs(
                factor=factor,
                db_path=financials_db_path,
                df_weight=df_weight,
                sector_neutral_mode=sector_neutral_mode,
                inversed=inversed,
                period=None,  # æœŸé–“ãªã—
                winsorize=winsorize,
                winsorize_limits=winsorize_limits,
            )
            tasks.append(args)

    total_iterations = len(tasks)
    logger.info("Rank calculation started", tasks=total_iterations, mode=mode_desc)

    # ---------------------------------------------------------
    # 2. DBè¨­å®šã¨ä¸¦åˆ—å®Ÿè¡Œã®æº–å‚™
    # ---------------------------------------------------------
    # DBã®WALãƒ¢ãƒ¼ãƒ‰æœ‰åŠ¹åŒ– (æ›¸ãè¾¼ã¿é€Ÿåº¦å‘ä¸Š)
    with sqlite3.connect(financials_db_path) as conn:
        conn.execute("PRAGMA journal_mode=WAL;")
    # ä¸¦åˆ—ã®ãƒ—ãƒ­ã‚»ã‚¹æ•°ãŒå¤šã„ã¨ãƒ¡ãƒ¢ãƒªã‚’é£Ÿã†ã®ã§æ³¨æ„ (4-6ç¨‹åº¦ãŒå®‰å…¨åœ)
    max_workers = min(default_max_workers, total_iterations)

    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        # futureã‚’ã‚­ãƒ¼ã«ã—ã¦ã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿(factor, period)ã‚’ä¿å­˜ã™ã‚‹è¾æ›¸
        futures = {}

        for job_args in tasks:
            future = executor.submit(process_ranking_factor_worker, job_args)
            # ã‚¨ãƒ©ãƒ¼è¡¨ç¤ºç”¨ã«æƒ…å ±ã‚’è¨˜éŒ²ã—ã¦ãŠã
            futures[future] = (job_args.factor, job_args.period)

        # ---------------------------------------------------------
        # 3. çµæœã®å›åã¨DBä¿å­˜ (ãƒ¡ã‚¤ãƒ³ã‚¹ãƒ¬ãƒƒãƒ‰)
        # ---------------------------------------------------------
        for future in tqdm(
            as_completed(futures), total=total_iterations, desc="Rankè¨ˆç®—é€²æ—"
        ):
            # ã©ã®ã‚¿ã‚¹ã‚¯ã®çµæœã‹ç‰¹å®š
            current_factor, current_period = futures[future]
            task_label = (
                f"{current_factor}_{current_period}"
                if current_period
                else current_factor
            )

            try:
                results = future.result()
                if not results:
                    continue

                # DBæ›¸ãè¾¼ã¿(ç›´åˆ—å®Ÿè¡Œã§ãƒ­ãƒƒã‚¯å›é¿)
                for table_name, df_result in results:
                    # æ—¢å­˜ãƒ†ãƒ¼ãƒ–ãƒ«å‰Šé™¤
                    db_utils.delete_table_from_database(  # type: ignore[name-defined]
                        db_path=financials_db_path, table_name=table_name
                    )
                    # ä¿å­˜
                    store_to_database(
                        df=df_result,
                        db_path=financials_db_path,
                        table_name=table_name,
                        verbose=False,
                    )

                del results

            except Exception as e:
                logger.critical(
                    "Rank calculation critical error",
                    task=task_label,
                    error=str(e),
                    exc_info=True,
                )

        logger.info("All rank calculations and saves completed")


# ============================================================================================
def check_missing_value_and_fill_by_sector_median(
    df: pd.DataFrame, factor_list: list[str]
) -> pd.DataFrame:
    # -----------------------------------
    # æ¬ æçŠ¶æ³ã®ç¢ºèª(è£œå®Œå‰)
    # -----------------------------------
    missing_before = {}
    for factor in factor_list:
        missing_count = df[factor].isna().sum()
        missing_before[factor] = missing_count

    logger.info(
        "Missing value analysis (before fill)",
        stats={f: int(missing_before[f]) for f in factor_list},
        total_rows=len(df),
    )

    # -----------------------------------
    # å„ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã®æ¬ æå€¤ã‚’ã‚»ã‚¯ã‚¿ãƒ¼ä¸­å¤®å€¤ã§è£œå®Œ
    # -----------------------------------
    logger.debug("Filling missing values with sector median")
    df[factor_list] = df.groupby(["date", "GICS Sector"])[factor_list].transform(
        lambda x: x.fillna(x.median())
    )

    # -----------------------------------
    # è£œå®Œå¾Œã®æ¬ æçŠ¶æ³ã‚’ç¢ºèª
    # -----------------------------------
    missing_after_sector = {}
    remaining_missing = False

    for factor in factor_list:
        missing_count = df[factor].isna().sum()
        missing_after_sector[factor] = missing_count
        if missing_count > 0:
            remaining_missing = True

    logger.info(
        "Missing value analysis (after sector median fill)",
        stats={f: int(missing_after_sector[f]) for f in factor_list},
        filled={
            f: int(missing_before[f] - missing_after_sector[f]) for f in factor_list
        },
    )

    # -----------------------------------
    # ã‚»ã‚¯ã‚¿ãƒ¼è£œå®Œã§åŸ‹ã¾ã‚‰ãªã‹ã£ãŸæ¬ æã‚’å…¨ä½“ä¸­å¤®å€¤ã§å†è£œå®Œ
    # -----------------------------------
    if remaining_missing:
        logger.info("Re-filling remaining missing values with overall median")

        for factor in factor_list:
            if df[factor].isna().sum() > 0:
                # æ—¥ä»˜ã”ã¨ã®å…¨ä½“ä¸­å¤®å€¤ã§è£œå®Œ
                df[factor] = df.groupby("date")[factor].transform(
                    lambda x: x.fillna(x.median())
                )

                # ãã‚Œã§ã‚‚åŸ‹ã¾ã‚‰ãªã„å ´åˆ(å…¨ä½“ãŒæ¬ æ)ã¯0.5(ä¸­ç«‹å€¤)
                overall_missing = df[factor].isna().sum()
                if overall_missing > 0:
                    logger.warning(
                        "Factor still missing after all fills, using neutral value 0.5",
                        factor=factor,
                        remaining=int(overall_missing),
                    )
                    df[factor] = df[factor].fillna(0.5)

    # -----------------------------------
    # æœ€çµ‚ç¢ºèª
    # -----------------------------------
    final_missing_total = 0
    final_stats = {}
    for factor in factor_list:
        missing_count = df[factor].isna().sum()
        final_missing_total += missing_count
        final_stats[factor] = int(missing_count)

    if final_missing_total == 0:
        logger.info("All missing values filled successfully")
    else:
        logger.warning(
            "Remaining missing values after all fills",
            total_remaining=final_missing_total,
            stats=final_stats,
        )

    return df


# ============================================================================================
def create_factor(
    df: pd.DataFrame, factor_name: str, blend_weight: dict
) -> pd.DataFrame:
    logger.info(
        "Factor calculation started",
        factor=factor_name,
        weights={k: round(v, 4) for k, v in blend_weight.items()},
    )

    # åŠ é‡å¹³å‡ã§Composite Scoreã‚’è¨ˆç®—
    df[f"{factor_name}_Score"] = sum(
        df[indicator] * weight for indicator, weight in blend_weight.items()
    )

    score_col = f"{factor_name}_Score"
    logger.info(
        "Score calculation completed",
        factor=factor_name,
        mean=round(df[score_col].mean(), 4),
        std=round(df[score_col].std(), 4),
        min=round(df[score_col].min(), 4),
        max=round(df[score_col].max(), 4),
    )

    # -----------------------------------
    # ãƒ©ãƒ³ã‚¯åŒ–
    # -----------------------------------
    df[f"{factor_name}_Score_Rank"] = df.groupby("date")[
        f"{factor_name}_Score"
    ].transform(
        lambda x: pd.qcut(
            x,
            q=5,
            labels=["rank5", "rank4", "rank3", "rank2", "rank1"],
            duplicates="drop",
        )
    )

    # ãƒ©ãƒ³ã‚¯åˆ†å¸ƒ
    rank_dist = df[f"{factor_name}_Score_Rank"].value_counts().sort_index()
    logger.info(
        "Rank distribution",
        factor=factor_name,
        distribution={str(k): int(v) for k, v in rank_dist.items()},
    )

    return df
