# é‡‘èãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå…±é€šå‡¦ç†ã‚¬ã‚¤ãƒ‰

ã“ã®ã‚¬ã‚¤ãƒ‰ã¯ã€ãƒ†ãƒ¼ãƒåˆ¥ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆfinance-news-{theme}ï¼‰ã®å…±é€šå‡¦ç†ã‚’å®šç¾©ã—ã¾ã™ã€‚

## ğŸš¨ æœ€é‡è¦: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ï¼ˆPhase 0ï¼‰

> **å‚ç…§**: `.claude/rules/subagent-data-passing.md`

ã‚µãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒå‡¦ç†ã‚’é–‹å§‹ã™ã‚‹å‰ã«ã€**å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®å®Œå…¨æ€§ã‚’å¿…ãšæ¤œè¨¼**ã™ã‚‹ã“ã¨ã€‚

### å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒã‚§ãƒƒã‚¯

```python
def validate_article_data(article: dict) -> tuple[bool, str]:
    """è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã®å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æ¤œè¨¼ã™ã‚‹

    Parameters
    ----------
    article : dict
        æ¤œè¨¼å¯¾è±¡ã®è¨˜äº‹ãƒ‡ãƒ¼ã‚¿

    Returns
    -------
    tuple[bool, str]
        (æ¤œè¨¼æˆåŠŸ, ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸)
    """

    required_fields = ["title", "link", "published", "summary"]

    for field in required_fields:
        if field not in article or not article[field]:
            return False, f"å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ '{field}' ãŒã‚ã‚Šã¾ã›ã‚“"

    # URLã®å½¢å¼ãƒã‚§ãƒƒã‚¯
    if not article["link"].startswith(("http://", "https://")):
        return False, f"ç„¡åŠ¹ãªURLå½¢å¼: {article['link']}"

    return True, ""


def validate_input_data(data: dict) -> tuple[bool, list[str]]:
    """å…¥åŠ›ãƒ‡ãƒ¼ã‚¿å…¨ä½“ã‚’æ¤œè¨¼ã™ã‚‹

    Parameters
    ----------
    data : dict
        ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¾ãŸã¯ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å—ã‘å–ã£ãŸãƒ‡ãƒ¼ã‚¿

    Returns
    -------
    tuple[bool, list[str]]
        (æ¤œè¨¼æˆåŠŸ, ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆ)
    """

    errors = []

    # 1. rss_items ã¾ãŸã¯ articles ã®å­˜åœ¨ç¢ºèª
    articles = data.get("rss_items") or data.get("articles") or []
    if not articles:
        errors.append("è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
        return False, errors

    # 2. å„è¨˜äº‹ã®å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æ¤œè¨¼
    for i, article in enumerate(articles):
        valid, msg = validate_article_data(article)
        if not valid:
            errors.append(f"è¨˜äº‹[{i}]: {msg}")

    # 3. ç°¡ç•¥åŒ–ãƒ‡ãƒ¼ã‚¿ã®æ¤œå‡ºï¼ˆè­¦å‘Šï¼‰
    if isinstance(articles[0], str):
        errors.append("ãƒ‡ãƒ¼ã‚¿ãŒæ–‡å­—åˆ—å½¢å¼ã§ã™ã€‚JSONå½¢å¼ã®å®Œå…¨ãªãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™")

    return len(errors) == 0, errors
```

### æ¤œè¨¼å¤±æ•—æ™‚ã®å¯¾å¿œ

```python
# Phase 0: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
valid, errors = validate_input_data(input_data)

if not valid:
    # ã‚¨ãƒ©ãƒ¼å ±å‘Šã—ã¦å‡¦ç†ä¸­æ–­
    error_report = "\n".join([f"  - {e}" for e in errors])
    print(f"""
## â›” å…¥åŠ›ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã‚¨ãƒ©ãƒ¼

å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ãŒä¸å®Œå…¨ãªãŸã‚å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã™ã€‚

### æ¤œå‡ºã•ã‚ŒãŸã‚¨ãƒ©ãƒ¼
{error_report}

### å¿…è¦ãªãƒ‡ãƒ¼ã‚¿å½¢å¼

è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã«ã¯ä»¥ä¸‹ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒå¿…é ˆã§ã™:
- `title`: è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«
- `link`: å…ƒè¨˜äº‹ã®URLï¼ˆ**çµ¶å¯¾ã«çœç•¥ç¦æ­¢**ï¼‰
- `published`: å…¬é–‹æ—¥æ™‚
- `summary`: è¨˜äº‹è¦ç´„

### å‚ç…§
- `.claude/rules/subagent-data-passing.md`
""")
    # å‡¦ç†ã‚’çµ‚äº†
    return
```

### ãƒ‡ãƒ¼ã‚¿å½¢å¼ã®ä¾‹

**æ­£ã—ã„å½¢å¼**:
```json
{
  "rss_items": [
    {
      "item_id": "60af4cc3-0a47-4cfb-ae89-ed8872209f5d",
      "title": "Trump threatens to sue JPMorgan Chase",
      "link": "https://www.cnbc.com/2026/01/17/trump-jpmorgan-chase-debanking.html",
      "published": "2026-01-18T13:47:50+00:00",
      "summary": "President Trump threatened to sue JPMorgan...",
      "content": null,
      "author": null,
      "fetched_at": "2026-01-18T22:40:08.589493+00:00"
    }
  ],
  "existing_issues": [...]
}
```

**ç¦æ­¢ã•ã‚Œã‚‹å½¢å¼**:
```
# âŒ ç°¡ç•¥åŒ–ã•ã‚ŒãŸãƒªã‚¹ãƒˆå½¢å¼ã¯çµ¶å¯¾ç¦æ­¢
1. "Trump threatens JPMorgan" - éŠ€è¡Œé–¢é€£
2. "BYD is a buy" - EVé–¢é€£
```

---

## å…±é€šè¨­å®š

- **Issueãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ**: `.github/ISSUE_TEMPLATE/news-article.yml`ï¼ˆYAMLå½¢å¼ã€GitHub UIç”¨ï¼‰
- **GitHub Project**: #15 (`PVT_kwHOBoK6AM4BMpw_`)
- **Statusãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰**: `PVTSSF_lAHOBoK6AM4BMpw_zg739ZE`
- **å…¬é–‹æ—¥æ™‚ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰**: `PVTF_lAHOBoK6AM4BMpw_zg8BzrI`ï¼ˆDateå‹ã€ã‚½ãƒ¼ãƒˆç”¨ï¼‰

## ä½¿ç”¨ãƒ„ãƒ¼ãƒ«

å„ã‚µãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ä»¥ä¸‹ã®ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™ï¼š

```yaml
tools:
  - Read              # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
  - Bash              # gh CLIå®Ÿè¡Œ
  - MCPSearch         # MCPãƒ„ãƒ¼ãƒ«æ¤œç´¢ãƒ»ãƒ­ãƒ¼ãƒ‰
  - mcp__rss__fetch_feed   # RSSãƒ•ã‚£ãƒ¼ãƒ‰æ›´æ–°
  - mcp__rss__get_items    # RSSè¨˜äº‹å–å¾—
```

## Phase 1: åˆæœŸåŒ–

### ã‚¹ãƒ†ãƒƒãƒ—1.1: MCPãƒ„ãƒ¼ãƒ«ã®ãƒ­ãƒ¼ãƒ‰

```python
def load_mcp_tools() -> bool:
    """MCPãƒ„ãƒ¼ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹"""

    try:
        # MCPSearchã§RSSãƒ„ãƒ¼ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
        MCPSearch(query="select:mcp__rss__fetch_feed")
        MCPSearch(query="select:mcp__rss__get_items")
        return True
    except Exception as e:
        ãƒ­ã‚°å‡ºåŠ›: f"è­¦å‘Š: MCPãƒ„ãƒ¼ãƒ«ã®ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}"
        ãƒ­ã‚°å‡ºåŠ›: "ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ä½¿ç”¨ã—ã¾ã™"
        return False
```

### ã‚¹ãƒ†ãƒƒãƒ—1.2: æ—¢å­˜Issueå–å¾—ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ï¼‰

**é‡è¦**: ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ`.tmp/news-collection-{timestamp}.json`ï¼‰ã‹ã‚‰æ—¢å­˜Issueã‚’èª­ã¿è¾¼ã‚€ã“ã¨ã€‚
ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ãŒæ—¢ã«å–å¾—æ¸ˆã¿ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã€**ç‹¬è‡ªã«`gh issue list`ã‚’å®Ÿè¡Œã—ãªã„**ã€‚

```python
def load_existing_issues_from_session(session_file: str) -> list[dict]:
    """ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ—¢å­˜Issueã‚’èª­ã¿è¾¼ã‚€

    Parameters
    ----------
    session_file : str
        ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ãŒä½œæˆã—ãŸä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹

    Returns
    -------
    list[dict]
        æ—¢å­˜Issueã®ãƒªã‚¹ãƒˆï¼ˆnumber, title, article_url, createdAtï¼‰
    """
    with open(session_file) as f:
        session_data = json.load(f)

    return session_data.get("existing_issues", [])
```

#### URLã®æŠ½å‡ºã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥

ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ã¯æ—¢å­˜Issueã‚’å–å¾—ã™ã‚‹éš›ã€**å„Issueæœ¬æ–‡ã‹ã‚‰è¨˜äº‹URLã‚’æŠ½å‡ºã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥**ã—ã¾ã™ã€‚

```bash
gh issue list \
    --repo YH-05/finance \
    --label "news" \
    --state all \
    --limit 500 \
    --json number,title,body,createdAt
```

**URLæŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯ï¼ˆã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ã§å®Ÿè¡Œï¼‰**:

```python
import re

def extract_article_url_from_body(body: str) -> str | None:
    """Issueæœ¬æ–‡ã‹ã‚‰æƒ…å ±æºURLã‚’æŠ½å‡ºã™ã‚‹

    Parameters
    ----------
    body : str
        Issueæœ¬æ–‡ï¼ˆMarkdownï¼‰

    Returns
    -------
    str | None
        æŠ½å‡ºã—ãŸè¨˜äº‹URLã€ã¾ãŸã¯ None

    Notes
    -----
    Issueæœ¬æ–‡ã®ã€Œæƒ…å ±æºURLã€å¿…é ˆã€‘ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰URLã‚’æŠ½å‡ºã™ã‚‹ã€‚
    ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ:
        ### æƒ…å ±æºURLã€å¿…é ˆã€‘
        > âš ï¸ ã“ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯å¿…é ˆã§ã™...
        https://example.com/article

    URLæŠ½å‡ºãƒ«ãƒ¼ãƒ«:
    1. ã€Œæƒ…å ±æºURLã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ä»¥é™ã‚’å¯¾è±¡
    2. https:// ã¾ãŸã¯ http:// ã§å§‹ã¾ã‚‹URLã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£
    3. ç©ºç™½ãƒ»æ”¹è¡Œã§çµ‚äº†
    """

    if not body:
        return None

    # æƒ…å ±æºURLã‚»ã‚¯ã‚·ãƒ§ãƒ³ä»¥é™ã‚’æŠ½å‡º
    url_section_match = re.search(
        r'###\s*æƒ…å ±æºURL.*?\n(.*?)(?=\n###|\Z)',
        body,
        re.DOTALL | re.IGNORECASE
    )

    if url_section_match:
        section_text = url_section_match.group(1)
        # URLã‚’æŠ½å‡ºï¼ˆhttps:// ã¾ãŸã¯ http:// ã§å§‹ã¾ã‚‹ï¼‰
        url_match = re.search(
            r'(https?://[^\s<>\[\]"\'\)]+)',
            section_text
        )
        if url_match:
            return url_match.group(1).rstrip('.,;:')

    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æœ¬æ–‡å…¨ä½“ã‹ã‚‰URLã‚’æ¤œç´¢
    url_match = re.search(
        r'(https?://[^\s<>\[\]"\'\)]+)',
        body
    )
    if url_match:
        return url_match.group(1).rstrip('.,;:')

    return None


def prepare_existing_issues_with_urls(raw_issues: list[dict]) -> list[dict]:
    """æ—¢å­˜Issueã‹ã‚‰URLã‚’æŠ½å‡ºã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹

    Parameters
    ----------
    raw_issues : list[dict]
        gh issue list ã§å–å¾—ã—ãŸç”Ÿã®Issueãƒªã‚¹ãƒˆ

    Returns
    -------
    list[dict]
        article_url ã‚’è¿½åŠ ã—ãŸIssueãƒªã‚¹ãƒˆ
    """
    result = []
    for issue in raw_issues:
        article_url = extract_article_url_from_body(issue.get("body", ""))
        result.append({
            "number": issue["number"],
            "title": issue["title"],
            "article_url": article_url,  # ğŸš¨ è¨˜äº‹URLï¼ˆIssueã® url ã§ã¯ãªã„ï¼‰
            "createdAt": issue.get("createdAt"),
        })
    return result
```

### ã‚¹ãƒ†ãƒƒãƒ—1.3: çµ±è¨ˆã‚«ã‚¦ãƒ³ã‚¿åˆæœŸåŒ–

```python
# çµ±è¨ˆã‚«ã‚¦ãƒ³ã‚¿ï¼ˆå¿…ãšå…¨é …ç›®ã‚’åˆæœŸåŒ–ã™ã‚‹ã“ã¨ï¼‰
stats = {
    "processed": 0,       # å–å¾—ã—ãŸè¨˜äº‹ç·æ•°
    "date_filtered": 0,   # å…¬é–‹æ—¥æ™‚ãƒ•ã‚£ãƒ«ã‚¿ã§ã‚¹ã‚­ãƒƒãƒ—ã•ã‚ŒãŸä»¶æ•°
    "matched": 0,         # ãƒ†ãƒ¼ãƒã«ãƒãƒƒãƒã—ãŸä»¶æ•°
    "excluded": 0,        # é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ã‚¹ã‚­ãƒƒãƒ—ã•ã‚ŒãŸä»¶æ•°
    "duplicates": 0,      # ğŸš¨ é‡è¤‡ã§ã‚¹ã‚­ãƒƒãƒ—ã•ã‚ŒãŸä»¶æ•°ï¼ˆå¿…é ˆã‚«ã‚¦ãƒ³ãƒˆï¼‰
    "skipped_no_url": 0,  # URLãªã—ã§ã‚¹ã‚­ãƒƒãƒ—ã•ã‚ŒãŸä»¶æ•°
    "created": 0,         # æ–°è¦ä½œæˆã—ãŸIssueæ•°
    "failed": 0,          # ä½œæˆå¤±æ•—ã—ãŸä»¶æ•°
}

# é‡è¤‡ã—ãŸè¨˜äº‹ã®ãƒªã‚¹ãƒˆï¼ˆãƒ¬ãƒãƒ¼ãƒˆç”¨ï¼‰
duplicate_articles = []  # [{"title": "...", "url": "...", "existing_issue": 123}, ...]
```

## Phase 2: RSSå–å¾—ï¼ˆãƒ•ã‚£ãƒ¼ãƒ‰ç›´æ¥å–å¾—ï¼‰

**é‡è¦**: å„ã‚µãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯è‡ªåˆ†ã®æ‹…å½“ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰ç›´æ¥è¨˜äº‹ã‚’å–å¾—ã—ã¾ã™ã€‚

### ã‚¹ãƒ†ãƒƒãƒ—2.1: æ‹…å½“ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰ã®å–å¾—

```python
def fetch_assigned_feeds(assigned_feeds: list[dict]) -> list[dict]:
    """æ‹…å½“ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰è¨˜äº‹ã‚’å–å¾—ã™ã‚‹

    Parameters
    ----------
    assigned_feeds : list[dict]
        æ‹…å½“ãƒ•ã‚£ãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆï¼ˆfeed_id, titleã‚’å«ã‚€ï¼‰

    Returns
    -------
    list[dict]
        å–å¾—ã—ãŸè¨˜äº‹ã®ãƒªã‚¹ãƒˆ
    """

    all_items = []

    for feed in assigned_feeds:
        feed_id = feed["feed_id"]
        feed_title = feed["title"]

        try:
            # Step 1: ãƒ•ã‚£ãƒ¼ãƒ‰ã‚’æœ€æ–°åŒ–
            mcp__rss__fetch_feed(feed_id=feed_id)

            # Step 2: è¨˜äº‹ã‚’å–å¾—ï¼ˆ24æ™‚é–“ä»¥å†…ï¼‰
            items = mcp__rss__get_items(
                feed_id=feed_id,
                hours=24,
                limit=50
            )

            # ãƒ•ã‚£ãƒ¼ãƒ‰æƒ…å ±ã‚’ä»˜åŠ 
            for item in items:
                item["feed_source"] = feed_title
                item["feed_id"] = feed_id

            all_items.extend(items)
            ãƒ­ã‚°å‡ºåŠ›: f"å–å¾—å®Œäº†: {feed_title} ({len(items)}ä»¶)"

        except Exception as e:
            ãƒ­ã‚°å‡ºåŠ›: f"è­¦å‘Š: ãƒ•ã‚£ãƒ¼ãƒ‰å–å¾—å¤±æ•—: {feed_title}: {e}"
            # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’è©¦è¡Œ
            local_items = load_from_local(feed_id, feed_title)
            all_items.extend(local_items)

    return all_items
```

### ã‚¹ãƒ†ãƒƒãƒ—2.2: ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

MCPãƒ„ãƒ¼ãƒ«ãŒåˆ©ç”¨ã§ããªã„å ´åˆã€ãƒ­ãƒ¼ã‚«ãƒ«ã«ä¿å­˜ã•ã‚ŒãŸRSSãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

```python
def load_from_local(feed_id: str, feed_title: str) -> list[dict]:
    """ãƒ­ãƒ¼ã‚«ãƒ«ã®RSSãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è¨˜äº‹ã‚’å–å¾—ã™ã‚‹

    Parameters
    ----------
    feed_id : str
        ãƒ•ã‚£ãƒ¼ãƒ‰ID
    feed_title : str
        ãƒ•ã‚£ãƒ¼ãƒ‰åï¼ˆãƒ­ã‚°ç”¨ï¼‰

    Returns
    -------
    list[dict]
        å–å¾—ã—ãŸè¨˜äº‹ã®ãƒªã‚¹ãƒˆ
    """

    local_path = f"data/raw/rss/{feed_id}/items.json"

    try:
        with open(local_path) as f:
            data = json.load(f)

        items = data.get("items", [])

        # 24æ™‚é–“ä»¥å†…ã®ã‚¢ã‚¤ãƒ†ãƒ ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿
        cutoff = datetime.now(timezone.utc) - timedelta(hours=24)
        recent_items = []

        for item in items:
            published = item.get("published")
            if published:
                try:
                    dt = datetime.fromisoformat(published.replace('Z', '+00:00'))
                    if dt >= cutoff:
                        item["feed_source"] = feed_title
                        item["feed_id"] = feed_id
                        recent_items.append(item)
                except ValueError:
                    continue

        ãƒ­ã‚°å‡ºåŠ›: f"ãƒ­ãƒ¼ã‚«ãƒ«ã‹ã‚‰å–å¾—: {feed_title} ({len(recent_items)}ä»¶)"
        return recent_items

    except FileNotFoundError:
        ãƒ­ã‚°å‡ºåŠ›: f"è­¦å‘Š: ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ãªã—: {local_path}"
        return []
    except json.JSONDecodeError as e:
        ãƒ­ã‚°å‡ºåŠ›: f"è­¦å‘Š: JSONãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {local_path}: {e}"
        return []
```

## Phase 2.5: å…¬é–‹æ—¥æ™‚ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã€å¿…é ˆã€‘

**é‡è¦**: `--since`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§æŒ‡å®šã•ã‚ŒãŸæœŸé–“å†…ã®è¨˜äº‹ã®ã¿ã‚’å‡¦ç†å¯¾è±¡ã¨ã—ã¾ã™ã€‚

### ã‚¹ãƒ†ãƒƒãƒ—2.5.1: --sinceãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è§£æ

```python
def parse_since_param(since: str) -> int:
    """--sinceãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ—¥æ•°ã«å¤‰æ›

    Parameters
    ----------
    since : str
        æœŸé–“æŒ‡å®šï¼ˆä¾‹: "1d", "3d", "7d"ï¼‰

    Returns
    -------
    int
        æ—¥æ•°

    Examples
    --------
    >>> parse_since_param("1d")
    1
    >>> parse_since_param("3d")
    3
    >>> parse_since_param("7d")
    7
    """

    if since.endswith("d"):
        try:
            return int(since[:-1])
        except ValueError:
            pass

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1æ—¥
    return 1
```

### ã‚¹ãƒ†ãƒƒãƒ—2.5.2: å…¬é–‹æ—¥æ™‚ã«ã‚ˆã‚‹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°

```python
from datetime import datetime, timedelta, timezone

def filter_by_published_date(
    items: list[dict],
    since_days: int,
) -> tuple[list[dict], int]:
    """å…¬é–‹æ—¥æ™‚ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°

    Parameters
    ----------
    items : list[dict]
        RSSè¨˜äº‹ãƒªã‚¹ãƒˆ
    since_days : int
        ç¾åœ¨æ—¥æ™‚ã‹ã‚‰é¡ã‚‹æ—¥æ•°

    Returns
    -------
    tuple[list[dict], int]
        (ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®è¨˜äº‹ãƒªã‚¹ãƒˆ, æœŸé–“å¤–ã§ã‚¹ã‚­ãƒƒãƒ—ã•ã‚ŒãŸä»¶æ•°)

    Notes
    -----
    - published ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯è¨˜äº‹ã®å…¬é–‹æ—¥æ™‚ï¼ˆRSSã®pubDateï¼‰
    - published ãŒãªã„å ´åˆã¯ fetched_at ã§ä»£æ›¿åˆ¤å®š
    - ä¸¡æ–¹ãªã„å ´åˆã¯å‡¦ç†å¯¾è±¡ã«å«ã‚ã‚‹ï¼ˆé™¤å¤–ã—ãªã„ï¼‰
    """

    cutoff = datetime.now(timezone.utc) - timedelta(days=since_days)
    filtered = []
    skipped = 0

    for item in items:
        # å…¬é–‹æ—¥æ™‚ã‚’å–å¾—ï¼ˆpublished â†’ fetched_at ã®é †ã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        date_str = item.get("published") or item.get("fetched_at")

        if not date_str:
            # æ—¥æ™‚æƒ…å ±ãŒãªã„å ´åˆã¯å‡¦ç†å¯¾è±¡ã«å«ã‚ã‚‹
            filtered.append(item)
            continue

        try:
            # ISO 8601å½¢å¼ã‚’ãƒ‘ãƒ¼ã‚¹
            dt = datetime.fromisoformat(date_str.replace('Z', '+00:00'))

            if dt >= cutoff:
                filtered.append(item)
            else:
                skipped += 1
                ãƒ­ã‚°å‡ºåŠ›: f"æœŸé–“å¤–ã‚¹ã‚­ãƒƒãƒ—: {item.get('title', 'ã‚¿ã‚¤ãƒˆãƒ«ãªã—')} (å…¬é–‹: {date_str})"

        except ValueError:
            # ãƒ‘ãƒ¼ã‚¹å¤±æ•—æ™‚ã¯å‡¦ç†å¯¾è±¡ã«å«ã‚ã‚‹
            filtered.append(item)

    ãƒ­ã‚°å‡ºåŠ›: f"å…¬é–‹æ—¥æ™‚ãƒ•ã‚£ãƒ«ã‚¿: {len(items)}ä»¶ â†’ {len(filtered)}ä»¶ (éå»{since_days}æ—¥ä»¥å†…)"
    return filtered, skipped
```

### ã‚¹ãƒ†ãƒƒãƒ—2.5.3: ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ

å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯RSSå–å¾—å¾Œã€ãƒ†ãƒ¼ãƒåˆ¤å®šå‰ã«ä»¥ä¸‹ã‚’å®Ÿè¡Œ:

```python
# --since ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ‘ãƒ¼ã‚¹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1dï¼‰
since_days = parse_since_param(args.get("since", "1d"))

# å…¬é–‹æ—¥æ™‚ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
items, date_skipped = filter_by_published_date(items, since_days)

# çµ±è¨ˆã«è¨˜éŒ²
stats["date_filtered"] = date_skipped
```

## Phase 3: AIåˆ¤æ–­ã«ã‚ˆã‚‹ãƒ†ãƒ¼ãƒåˆ†é¡

**é‡è¦**: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°ã¯ä½¿ç”¨ã—ã¾ã›ã‚“ã€‚**AIãŒè¨˜äº‹ã®å†…å®¹ã‚’èª­ã¿å–ã‚Šã€ãƒ†ãƒ¼ãƒã«è©²å½“ã™ã‚‹ã‹åˆ¤æ–­**ã—ã¾ã™ã€‚

### ã‚¹ãƒ†ãƒƒãƒ—3.1: AIåˆ¤æ–­ã«ã‚ˆã‚‹ãƒ†ãƒ¼ãƒåˆ¤å®š

å„è¨˜äº‹ã«ã¤ã„ã¦ã€ã‚¿ã‚¤ãƒˆãƒ«ã¨è¦ç´„ï¼ˆsummaryï¼‰ã‚’èª­ã¿å–ã‚Šã€ä»¥ä¸‹ã®åŸºæº–ã§ãƒ†ãƒ¼ãƒã«è©²å½“ã™ã‚‹ã‹åˆ¤æ–­ã—ã¾ã™ã€‚

**ãƒ†ãƒ¼ãƒåˆ¥åˆ¤å®šåŸºæº–**:

| ãƒ†ãƒ¼ãƒ | åˆ¤å®šåŸºæº– |
|--------|----------|
| **Index** | æ ªä¾¡æŒ‡æ•°ï¼ˆæ—¥çµŒå¹³å‡ã€TOPIXã€S&P500ã€ãƒ€ã‚¦ã€ãƒŠã‚¹ãƒ€ãƒƒã‚¯ç­‰ï¼‰ã®å‹•å‘ã€å¸‚å ´å…¨ä½“ã®ä¸Šæ˜‡/ä¸‹è½ã€ETFé–¢é€£ |
| **Stock** | å€‹åˆ¥ä¼æ¥­ã®æ±ºç®—ç™ºè¡¨ã€æ¥­ç¸¾äºˆæƒ³ã€M&Aã€è²·åã€ææºã€æ ªå¼å…¬é–‹ã€çµŒå–¶é™£ã®å¤‰æ›´ |
| **Sector** | ç‰¹å®šæ¥­ç•Œï¼ˆéŠ€è¡Œã€ä¿é™ºã€è‡ªå‹•è»Šã€åŠå°ä½“ã€ãƒ˜ãƒ«ã‚¹ã‚±ã‚¢ã€ã‚¨ãƒãƒ«ã‚®ãƒ¼ç­‰ï¼‰ã®å‹•å‘ã€è¦åˆ¶å¤‰æ›´ |
| **Macro** | é‡‘èæ”¿ç­–ï¼ˆé‡‘åˆ©ã€é‡çš„ç·©å’Œï¼‰ã€ä¸­å¤®éŠ€è¡Œï¼ˆFedã€æ—¥éŠ€ã€ECBï¼‰ã€çµŒæ¸ˆæŒ‡æ¨™ï¼ˆGDPã€CPIã€é›‡ç”¨çµ±è¨ˆï¼‰ã€ç‚ºæ›¿ |
| **AI** | AIæŠ€è¡“ã€æ©Ÿæ¢°å­¦ç¿’ã€ç”ŸæˆAIã€AIä¼æ¥­ï¼ˆOpenAIã€NVIDIAç­‰ï¼‰ã€AIæŠ•è³‡ã€AIè¦åˆ¶ |

**åˆ¤å®šãƒ—ãƒ­ã‚»ã‚¹**:

```
[1] è¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ã¨è¦ç´„ã‚’èª­ã‚€
    â†“
[2] è¨˜äº‹ã®ä¸»é¡Œã‚’ç†è§£ã™ã‚‹
    â†“
[3] ä¸Šè¨˜ãƒ†ãƒ¼ãƒåˆ¤å®šåŸºæº–ã«ç…§ã‚‰ã—ã¦è©²å½“ã™ã‚‹ã‹åˆ¤æ–­
    â†“
[4] è©²å½“ã™ã‚‹å ´åˆ â†’ Phase 2.2ã¸
    è©²å½“ã—ãªã„å ´åˆ â†’ ã‚¹ã‚­ãƒƒãƒ—
```

**åˆ¤å®šä¾‹**:

| è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ« | AIã®åˆ¤æ–­ | ãƒ†ãƒ¼ãƒ |
|------------|---------|--------|
| "S&P 500 hits new record high amid tech rally" | æ ªä¾¡æŒ‡æ•°ã®å‹•å‘ã«ã¤ã„ã¦ â†’ è©²å½“ | Index |
| "Fed signals rate cut in March meeting" | é‡‘èæ”¿ç­–ãƒ»ä¸­å¤®éŠ€è¡Œã®å‹•å‘ â†’ è©²å½“ | Macro |
| "Apple reports Q4 earnings beat" | å€‹åˆ¥ä¼æ¥­ã®æ±ºç®—ç™ºè¡¨ â†’ è©²å½“ | Stock |
| "Banks face new capital requirements" | éŠ€è¡Œã‚»ã‚¯ã‚¿ãƒ¼ã®è¦åˆ¶ â†’ è©²å½“ | Sector |
| "OpenAI launches new model capabilities" | AIä¼æ¥­ã®å‹•å‘ â†’ è©²å½“ | AI |
| "Celebrity launches new clothing line" | é‡‘èãƒ»çµŒæ¸ˆã¨ç„¡é–¢ä¿‚ â†’ éè©²å½“ | - |

### ã‚¹ãƒ†ãƒƒãƒ—3.2: é™¤å¤–åˆ¤æ–­

ä»¥ä¸‹ã®ã‚«ãƒ†ã‚´ãƒªã«è©²å½“ã™ã‚‹è¨˜äº‹ã¯é™¤å¤–ã—ã¾ã™ï¼ˆé‡‘èãƒ†ãƒ¼ãƒã«é–¢é€£ã™ã‚‹å ´åˆã‚’é™¤ãï¼‰:

- **ã‚¹ãƒãƒ¼ãƒ„**: è©¦åˆçµæœã€é¸æ‰‹ç§»ç±ãªã©ï¼ˆãŸã ã—ã€ã‚¹ãƒãƒ¼ãƒ„é–¢é€£ä¼æ¥­ã®æ±ºç®—ç­‰ã¯å¯¾è±¡ï¼‰
- **ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ†ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆ**: æ˜ ç”»ã€éŸ³æ¥½ã€èŠ¸èƒ½ãƒ‹ãƒ¥ãƒ¼ã‚¹
- **æ”¿æ²»**: é¸æŒ™ã€å†…é–£é–¢é€£ï¼ˆãŸã ã—ã€é‡‘èæ”¿ç­–ãƒ»è¦åˆ¶ã«é–¢é€£ã™ã‚‹å ´åˆã¯å¯¾è±¡ï¼‰
- **ä¸€èˆ¬ãƒ‹ãƒ¥ãƒ¼ã‚¹**: äº‹æ•…ã€ç½å®³ã€çŠ¯ç½ª

### ã‚¹ãƒ†ãƒƒãƒ—3.3: é‡è¤‡ãƒã‚§ãƒƒã‚¯

> **ğŸš¨ é‡è¦: é‡è¤‡ãƒã‚§ãƒƒã‚¯ã¯æœ€åˆã«å®Ÿè¡Œã™ã‚‹ã“ã¨ ğŸš¨**
>
> ãƒ†ãƒ¼ãƒãƒãƒƒãƒãƒ³ã‚°å¾Œã§ã¯ãªãã€**RSSå–å¾—ç›´å¾Œï¼ˆå…¬é–‹æ—¥æ™‚ãƒ•ã‚£ãƒ«ã‚¿å¾Œï¼‰**ã«é‡è¤‡ãƒã‚§ãƒƒã‚¯ã‚’è¡Œã†ã“ã¨ã€‚
> ã“ã‚Œã«ã‚ˆã‚Šã€ç•°ãªã‚‹ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰å–å¾—ã•ã‚ŒãŸåŒä¸€è¨˜äº‹ã‚’æ—©æœŸã«é™¤å¤–ã§ãã‚‹ã€‚

```python
# é™¤å»å¯¾è±¡ã®ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
# ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹å‹ï¼ˆæœ«å°¾ "_" ã§åˆ¤å®šï¼‰ã¨å®Œå…¨ä¸€è‡´å‹ã®ä¸¡æ–¹ã‚’å«ã‚€
TRACKING_PARAMS = {
    # ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹å‹ï¼ˆæ—¢å­˜ï¼‰
    "utm_", "guce_",
    # å®Œå…¨ä¸€è‡´å‹ï¼ˆæ—¢å­˜ï¼‰
    "ncid", "fbclid", "gclid",
    # å®Œå…¨ä¸€è‡´å‹ï¼ˆæ–°è¦è¿½åŠ ï¼‰
    "ref", "source", "campaign", "si", "mc_cid", "mc_eid",
    "sref", "taid", "mod", "cmpid",
}


def normalize_url(url: str) -> str:
    """URLã‚’æ­£è¦åŒ–ã—ã¦æ¯”è¼ƒã—ã‚„ã™ãã™ã‚‹ï¼ˆå¼·åŒ–ç‰ˆï¼‰

    Parameters
    ----------
    url : str
        æ­£è¦åŒ–å¯¾è±¡ã®URL

    Returns
    -------
    str
        æ­£è¦åŒ–ã•ã‚ŒãŸURL

    Notes
    -----
    æ­£è¦åŒ–ãƒ«ãƒ¼ãƒ«ï¼ˆæ¯”è¼ƒæ™‚ã®ã¿é©ç”¨ã€‚ä¿å­˜URLã¯å¤‰æ›´ã—ãªã„ï¼‰:
    - æœ«å°¾ã®ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’é™¤å»
    - ãƒ›ã‚¹ãƒˆéƒ¨åˆ†ã®å°æ–‡å­—åŒ–
    - ``www.`` ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã®é™¤å»
    - ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆï¼ˆ``#section``ï¼‰ã®é™¤å»
    - æœ«å°¾ ``/index.html`` ã®é™¤å»
    - ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®é™¤å»ï¼ˆTRACKING_PARAMS å‚ç…§ï¼‰

    é‡è¦: ã“ã®é–¢æ•°ã¯é‡è¤‡ãƒã‚§ãƒƒã‚¯ã®æ¯”è¼ƒç”¨ã§ã™ã€‚
    Issueä½œæˆæ™‚ã«ä½¿ç”¨ã™ã‚‹URLã¯ã€RSSã‚ªãƒªã‚¸ãƒŠãƒ«ã® link ã‚’ãã®ã¾ã¾ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
    """
    if not url:
        return ""

    import urllib.parse

    # æœ«å°¾ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’é™¤å»
    url = url.rstrip('/')

    # URLã‚’ãƒ‘ãƒ¼ã‚¹
    parsed = urllib.parse.urlparse(url)

    # ãƒ›ã‚¹ãƒˆéƒ¨åˆ†: å°æ–‡å­—åŒ– + www. é™¤å»
    netloc = parsed.netloc.lower()
    if netloc.startswith("www."):
        netloc = netloc[4:]

    # ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆé™¤å»
    parsed = parsed._replace(fragment="")

    # æœ«å°¾ /index.html é™¤å»
    path = parsed.path
    if path.endswith("/index.html"):
        path = path[:-len("/index.html")]
    parsed = parsed._replace(path=path)

    # ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‹ã‚‰ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ç”¨ã‚’é™¤å»
    if parsed.query:
        params = urllib.parse.parse_qs(parsed.query)
        filtered_params = {
            k: v for k, v in params.items()
            if not any(
                k.startswith(prefix) if prefix.endswith("_") else k == prefix
                for prefix in TRACKING_PARAMS
            )
        }
        new_query = urllib.parse.urlencode(filtered_params, doseq=True)
        parsed = parsed._replace(query=new_query)

    # å†æ§‹ç¯‰
    normalized = urllib.parse.urlunparse(parsed._replace(netloc=netloc))

    return normalized


def calculate_title_similarity(title1: str, title2: str) -> float:
    """ã‚¿ã‚¤ãƒˆãƒ«ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—ï¼ˆJaccardä¿‚æ•°ï¼‰"""

    words1 = set(title1.lower().split())
    words2 = set(title2.lower().split())

    if not words1 or not words2:
        return 0.0

    common = words1.intersection(words2)
    total = words1.union(words2)

    return len(common) / len(total)


def is_duplicate(
    new_item: dict,
    existing_issues: list[dict],
    threshold: float = 0.85
) -> tuple[bool, int | None, str | None]:
    """æ—¢å­˜Issueã¨é‡è¤‡ã—ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯

    Parameters
    ----------
    new_item : dict
        ãƒã‚§ãƒƒã‚¯å¯¾è±¡ã®è¨˜äº‹ï¼ˆlinkãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å¿…é ˆï¼‰
    existing_issues : list[dict]
        æ—¢å­˜ã®Issueãƒªã‚¹ãƒˆï¼ˆarticle_urlãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ä½¿ç”¨ï¼‰
    threshold : float
        ã‚¿ã‚¤ãƒˆãƒ«é¡ä¼¼åº¦ã®é–¾å€¤

    Returns
    -------
    tuple[bool, int | None, str | None]
        (é‡è¤‡åˆ¤å®š, æ—¢å­˜Issueç•ªå·, é‡è¤‡ç†ç”±)
        - é‡è¤‡ã®å ´åˆ: (True, issue_number, "URLä¸€è‡´" or "ã‚¿ã‚¤ãƒˆãƒ«é¡ä¼¼")
        - é‡è¤‡ãªã—ã®å ´åˆ: (False, None, None)

    Notes
    -----
    1. ã¾ãšURLå®Œå…¨ä¸€è‡´ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆæ­£è¦åŒ–å¾Œï¼‰
    2. æ¬¡ã«ã‚¿ã‚¤ãƒˆãƒ«é¡ä¼¼åº¦ã‚’ãƒã‚§ãƒƒã‚¯
    3. existing_issuesã¯ article_url ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æŒã¤ã“ã¨
       ï¼ˆã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ã§prepare_existing_issues_with_urls()å‡¦ç†æ¸ˆã¿ï¼‰
    """

    new_link = new_item.get('link', '')
    new_title = new_item.get('title', '')
    new_link_normalized = normalize_url(new_link)

    for issue in existing_issues:
        # â˜… article_url ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ä½¿ç”¨ï¼ˆbodyã‹ã‚‰ã§ã¯ãªãæŠ½å‡ºæ¸ˆã¿ï¼‰
        existing_url = issue.get('article_url', '')
        existing_url_normalized = normalize_url(existing_url)

        # URLå®Œå…¨ä¸€è‡´ï¼ˆæ­£è¦åŒ–å¾Œï¼‰
        if new_link_normalized and existing_url_normalized:
            if new_link_normalized == existing_url_normalized:
                return True, issue.get('number'), "URLä¸€è‡´"

        # ã‚¿ã‚¤ãƒˆãƒ«é¡ä¼¼åº¦ãƒã‚§ãƒƒã‚¯
        issue_title = issue.get('title', '')
        similarity = calculate_title_similarity(new_title, issue_title)

        if similarity >= threshold:
            return True, issue.get('number'), f"ã‚¿ã‚¤ãƒˆãƒ«é¡ä¼¼({similarity:.0%})"

    return False, None, None


def check_duplicates_and_count(
    items: list[dict],
    existing_issues: list[dict],
    stats: dict,
    duplicate_articles: list[dict],
) -> list[dict]:
    """é‡è¤‡ãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œã—ã€çµ±è¨ˆã‚’ã‚«ã‚¦ãƒ³ãƒˆã™ã‚‹

    Parameters
    ----------
    items : list[dict]
        ãƒã‚§ãƒƒã‚¯å¯¾è±¡ã®è¨˜äº‹ãƒªã‚¹ãƒˆ
    existing_issues : list[dict]
        æ—¢å­˜ã®Issueãƒªã‚¹ãƒˆ
    stats : dict
        çµ±è¨ˆã‚«ã‚¦ãƒ³ã‚¿ï¼ˆduplicatesã‚’æ›´æ–°ï¼‰
    duplicate_articles : list[dict]
        é‡è¤‡è¨˜äº‹ãƒªã‚¹ãƒˆï¼ˆè¿½è¨˜ã•ã‚Œã‚‹ï¼‰

    Returns
    -------
    list[dict]
        é‡è¤‡ã‚’é™¤ã„ãŸè¨˜äº‹ãƒªã‚¹ãƒˆ
    """

    non_duplicates = []

    for item in items:
        is_dup, issue_number, reason = is_duplicate(item, existing_issues)

        if is_dup:
            stats["duplicates"] += 1
            duplicate_articles.append({
                "title": item.get("title", ""),
                "url": item.get("link", ""),
                "existing_issue": issue_number,
                "reason": reason,
            })
            ãƒ­ã‚°å‡ºåŠ›: f"é‡è¤‡ã‚¹ã‚­ãƒƒãƒ—: {item.get('title', '')} â†’ Issue #{issue_number} ({reason})"
        else:
            non_duplicates.append(item)

    return non_duplicates
```

## Phase 4: ãƒãƒƒãƒæŠ•ç¨¿ï¼ˆarticle-fetcherã«å§”è­²ï¼‰

> **ğŸš¨ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåŠ¹ç‡åŒ–ã®ãŸã‚ã€Issueä½œæˆã‚’å«ã‚€å…¨å‡¦ç†ã‚’ã‚µãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«å§”è­²ã—ã¾ã™ ğŸš¨**
>
> è¨˜äº‹æœ¬æ–‡ã®å–å¾—ã€æ—¥æœ¬èªè¦ç´„ã®ç”Ÿæˆã€Issueä½œæˆã€Projectè¿½åŠ ã€Status/Dateè¨­å®šã¯
> ã™ã¹ã¦ `news-article-fetcher` ã‚µãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆSonnetï¼‰ãŒæ‹…å½“ã—ã¾ã™ã€‚
> ãƒ†ãƒ¼ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿è¨˜äº‹ã‚’ãƒãƒƒãƒåˆ†å‰²ã—ã¦å§”è­²ã™ã‚‹ã®ã¿ã§ã™ã€‚

### Phase 4 å‡¦ç†ãƒ•ãƒ­ãƒ¼æ¦‚è¦

```
Phase 4: ãƒãƒƒãƒæŠ•ç¨¿ï¼ˆarticle-fetcherã«å§”è­²ï¼‰
â”œâ”€â”€ URLå¿…é ˆãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
â”œâ”€â”€ 5ä»¶ãšã¤ãƒãƒƒãƒåˆ†å‰²ï¼ˆå…¬é–‹æ—¥æ™‚ã®æ–°ã—ã„é †ï¼‰
â””â”€â”€ å„ãƒãƒƒãƒ â†’ news-article-fetcherï¼ˆSonnetï¼‰
    â”œâ”€â”€ ãƒšã‚¤ã‚¦ã‚©ãƒ¼ãƒ«/JSäº‹å‰ãƒã‚§ãƒƒã‚¯ï¼ˆarticle_content_checker.pyï¼‰
    â”œâ”€â”€ ãƒã‚§ãƒƒã‚¯é€šé â†’ WebFetch â†’ è¦ç´„ç”Ÿæˆ
    â”œâ”€â”€ ãƒã‚§ãƒƒã‚¯ä¸é€šé â†’ ã‚¹ã‚­ãƒƒãƒ—ï¼ˆstatsè¨˜éŒ²ï¼‰
    â”œâ”€â”€ Issueä½œæˆ + closeï¼ˆ.github/ISSUE_TEMPLATE/news-article.yml æº–æ‹ ï¼‰
    â”œâ”€â”€ Projectè¿½åŠ 
    â”œâ”€â”€ Statusè¨­å®š
    â””â”€â”€ å…¬é–‹æ—¥æ™‚è¨­å®š
```

### ã‚¹ãƒ†ãƒƒãƒ—4.1: URLå¿…é ˆãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã€æŠ•ç¨¿å‰ãƒã‚§ãƒƒã‚¯ã€‘

> **ğŸš¨ Issueä½œæˆå‰ã«å¿…ãšå®Ÿè¡Œã™ã‚‹ã“ã¨ ğŸš¨**
>
> URLãŒå­˜åœ¨ã—ãªã„è¨˜äº‹ã¯**çµ¶å¯¾ã«Issueä½œæˆã—ã¦ã¯ã„ã‘ã¾ã›ã‚“**ã€‚
> ãƒãƒƒãƒåˆ†å‰²å‰ã«URLãªã—è¨˜äº‹ã‚’é™¤å¤–ã™ã‚‹ã“ã¨ã€‚

```python
def validate_url_for_issue(item: dict) -> tuple[bool, str | None]:
    """Issueä½œæˆå‰ã«URLã®å­˜åœ¨ã‚’æ¤œè¨¼ã™ã‚‹

    Parameters
    ----------
    item : dict
        RSSã‹ã‚‰å–å¾—ã—ãŸè¨˜äº‹ã‚¢ã‚¤ãƒ†ãƒ 

    Returns
    -------
    tuple[bool, str | None]
        (æ¤œè¨¼æˆåŠŸ, ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸)

    Notes
    -----
    - URLãŒãªã„è¨˜äº‹ã¯Issueä½œæˆã—ãªã„
    - ç©ºæ–‡å­—åˆ—ã‚‚URLãªã—ã¨ã—ã¦æ‰±ã†
    """

    url = item.get("link", "").strip()

    if not url:
        return False, f"URLãªã—: {item.get('title', 'ä¸æ˜')}"

    if not url.startswith(("http://", "https://")):
        return False, f"ç„¡åŠ¹ãªURLå½¢å¼: {url}"

    return True, None


# ä½¿ç”¨ä¾‹: ãƒãƒƒãƒåˆ†å‰²å‰ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
valid_items = []
for item in filtered_items:
    valid, error = validate_url_for_issue(item)
    if not valid:
        ãƒ­ã‚°å‡ºåŠ›: f"ã‚¹ã‚­ãƒƒãƒ—ï¼ˆURLå¿…é ˆé•åï¼‰: {error}"
        stats["skipped_no_url"] += 1
        continue
    valid_items.append(item)
```

### ã‚¹ãƒ†ãƒƒãƒ—4.2: ãƒãƒƒãƒå‡¦ç†

ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä½¿ç”¨é‡ã‚’å‰Šæ¸›ã™ã‚‹ãŸã‚ã€è¨˜äº‹ã‚’5ä»¶ãšã¤ `news-article-fetcher` ã«å§”è­²ã—ã¾ã™ã€‚

| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | å€¤ |
|-----------|-----|
| ãƒãƒƒãƒã‚µã‚¤ã‚º | 5ä»¶ |
| å‡¦ç†é †åº | å…¬é–‹æ—¥æ™‚ã®æ–°ã—ã„é † |
| å§”è­²å…ˆ | news-article-fetcherï¼ˆSonnetï¼‰ |
| å§”è­²ç¯„å›² | ãƒšã‚¤ã‚¦ã‚©ãƒ¼ãƒ«ãƒã‚§ãƒƒã‚¯ + WebFetch + è¦ç´„ç”Ÿæˆ + Issueä½œæˆ + Projectè¿½åŠ  + Status/Dateè¨­å®š |

#### ãƒãƒƒãƒå‡¦ç†ãƒ•ãƒ­ãƒ¼

```python
BATCH_SIZE = 5

# å…¬é–‹æ—¥æ™‚ã®æ–°ã—ã„é †ã«ã‚½ãƒ¼ãƒˆ
sorted_items = sorted(valid_items, key=lambda x: x.get("published", ""), reverse=True)

all_created = []
all_skipped = []

for i in range(0, len(sorted_items), BATCH_SIZE):
    batch = sorted_items[i:i + BATCH_SIZE]
    batch_num = (i // BATCH_SIZE) + 1
    ãƒ­ã‚°å‡ºåŠ›: f"ãƒãƒƒãƒ {batch_num} å‡¦ç†ä¸­... ({len(batch)}ä»¶)"

    # article-fetcher ã«å§”è­²
    result = Task(
        subagent_type="news-article-fetcher",
        description=f"ãƒãƒƒãƒ{batch_num}: è¨˜äº‹å–å¾—ãƒ»è¦ç´„ãƒ»Issueä½œæˆ",
        prompt=f"""ä»¥ä¸‹ã®è¨˜äº‹ã‚’å‡¦ç†ã—ã¦ãã ã•ã„ã€‚

å…¥åŠ›:
{json.dumps({
    "articles": [
        {
            "url": item["link"],
            "title": item["title"],
            "summary": item.get("summary", ""),
            "feed_source": item.get("feed_source", ""),
            "published": item.get("published", "")
        }
        for item in batch
    ],
    "issue_config": issue_config  # build_issue_config() ã§æ§‹ç¯‰æ¸ˆã¿
}, ensure_ascii=False, indent=2)}
""")

    # çµæœé›†ç´„
    all_created.extend(result.get("created_issues", []))
    all_skipped.extend(result.get("skipped", []))
    stats["created"] += result["stats"]["issue_created"]
    stats["failed"] += result["stats"]["issue_failed"]
```

#### ãƒãƒƒãƒé–“ã®çŠ¶æ…‹ç®¡ç†

- å„ãƒãƒƒãƒã®çµæœï¼ˆ`created_issues`, `skipped`ï¼‰ã¯ãƒ†ãƒ¼ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå´ã§é›†ç´„
- çµ±è¨ˆã‚«ã‚¦ãƒ³ã‚¿ï¼ˆ`stats`ï¼‰ã¯å…¨ãƒãƒƒãƒã§å…±æœ‰ãƒ»ç´¯ç©
- ãƒãƒƒãƒå¤±æ•—æ™‚ã‚‚æ¬¡ã®ãƒãƒƒãƒã¯ç¶™ç¶š
- é‡è¤‡ãƒã‚§ãƒƒã‚¯ã¯ãƒ†ãƒ¼ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå´ï¼ˆãƒãƒƒãƒåˆ†å‰²å‰ã® Phase 3ï¼‰ã§å®Œäº†æ¸ˆã¿

### ã‚¹ãƒ†ãƒƒãƒ—4.3: article-fetcher å…¥åŠ›ä»•æ§˜

#### articles[] ã®å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰

| ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ | å¿…é ˆ | èª¬æ˜ |
|-----------|------|------|
| `url` | Yes | å…ƒè¨˜äº‹URLï¼ˆRSSã®linkãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼‰ |
| `title` | Yes | è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ« |
| `summary` | Yes | RSSæ¦‚è¦ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰ |
| `feed_source` | Yes | ãƒ•ã‚£ãƒ¼ãƒ‰å |
| `published` | Yes | å…¬é–‹æ—¥æ™‚ï¼ˆISO 8601ï¼‰ |

#### issue_config ã®å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰

| ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ | èª¬æ˜ | ä¾‹ |
|-----------|------|-----|
| `theme_key` | ãƒ†ãƒ¼ãƒã‚­ãƒ¼ | `"index"` |
| `theme_label` | ãƒ†ãƒ¼ãƒæ—¥æœ¬èªå | `"æ ªä¾¡æŒ‡æ•°"` |
| `status_option_id` | Statusã®Option ID | `"3925acc3"` |
| `project_id` | Project ID | `"PVT_kwHOBoK6AM4BMpw_"` |
| `project_number` | Projectç•ªå· | `15` |
| `project_owner` | Projectã‚ªãƒ¼ãƒŠãƒ¼ | `"YH-05"` |
| `repo` | ãƒªãƒã‚¸ãƒˆãƒª | `"YH-05/finance"` |
| `status_field_id` | Statusãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ID | `"PVTSSF_lAHOBoK6AM4BMpw_zg739ZE"` |
| `published_date_field_id` | å…¬é–‹æ—¥ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ID | `"PVTF_lAHOBoK6AM4BMpw_zg8BzrI"` |

#### issue_config ã®æ§‹ç¯‰ãƒ‘ã‚¿ãƒ¼ãƒ³

ãƒ†ãƒ¼ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã® `config` ã¨ãƒ†ãƒ¼ãƒå›ºæœ‰è¨­å®šã‚’çµ„ã¿åˆã‚ã›ã¦ `issue_config` ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚

```python
def build_issue_config(
    session_data: dict,
    theme_key: str,
    theme_label: str,
    status_option_id: str,
) -> dict:
    """ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰issue_configã‚’æ§‹ç¯‰ã™ã‚‹

    Parameters
    ----------
    session_data : dict
        ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ãŒä½œæˆã—ãŸã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ãƒ¼ã‚¿
    theme_key : str
        ãƒ†ãƒ¼ãƒã‚­ãƒ¼ï¼ˆä¾‹: "index", "stock", "macro"ï¼‰
    theme_label : str
        ãƒ†ãƒ¼ãƒæ—¥æœ¬èªåï¼ˆä¾‹: "æ ªä¾¡æŒ‡æ•°", "å€‹åˆ¥éŠ˜æŸ„", "ãƒã‚¯ãƒ­çµŒæ¸ˆ"ï¼‰
    status_option_id : str
        GitHub Projectã®Statusãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®Option ID

    Returns
    -------
    dict
        article-fetcherã«æ¸¡ã™issue_config
    """

    config = session_data["config"]
    return {
        "theme_key": theme_key,
        "theme_label": theme_label,
        "status_option_id": status_option_id,
        "project_id": config["project_id"],
        "project_number": config["project_number"],
        "project_owner": config["project_owner"],
        "repo": "YH-05/finance",
        "status_field_id": config["status_field_id"],
        "published_date_field_id": config["published_date_field_id"],
    }
```

**ä½¿ç”¨ä¾‹**:

```python
# ãƒ†ãƒ¼ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå†…ã§ã®ä½¿ç”¨
issue_config = build_issue_config(
    session_data=session_data,
    theme_key="index",
    theme_label="æ ªä¾¡æŒ‡æ•°",
    status_option_id="3925acc3",
)
```

### ã‚¹ãƒ†ãƒƒãƒ—4.4: article-fetcher ã®æˆ»ã‚Šå€¤

article-fetcher ã¯å„ãƒãƒƒãƒå‡¦ç†å¾Œã€ä»¥ä¸‹ã®JSONå½¢å¼ã§çµæœã‚’è¿”å´ã—ã¾ã™ã€‚

```json
{
  "created_issues": [
    {
      "issue_number": 200,
      "issue_url": "https://github.com/YH-05/finance/issues/200",
      "title": "[æ ªä¾¡æŒ‡æ•°] S&P500ãŒéå»æœ€é«˜å€¤ã‚’æ›´æ–°",
      "article_url": "https://www.cnbc.com/...",
      "published_date": "2026-01-19"
    }
  ],
  "skipped": [
    {
      "url": "https://...",
      "title": "...",
      "reason": "ãƒšã‚¤ã‚¦ã‚©ãƒ¼ãƒ«æ¤œå‡º (Tier 3: 'subscribe to continue' æ¤œå‡º, æœ¬æ–‡320æ–‡å­—)"
    }
  ],
  "stats": {
    "total": 5,
    "content_check_passed": 4,
    "content_check_failed": 1,
    "fetch_success": 3,
    "fetch_failed": 1,
    "issue_created": 3,
    "issue_failed": 0,
    "skipped_paywall": 1,
    "skipped_format": 0
  }
}
```

> **ğŸš¨ URLè¨­å®šã®é‡è¦ãƒ«ãƒ¼ãƒ« ğŸš¨**: `created_issues[].article_url` ã¯
> RSSã‚ªãƒªã‚¸ãƒŠãƒ«ã®linkã‚’ãã®ã¾ã¾ä¿æŒã—ã¦ã„ã¾ã™ã€‚WebFetchã§ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆãŒ
> ç™ºç”Ÿã—ã¦ã‚‚ã€Issueè¨˜è¼‰ã®URLã¯ã“ã®å€¤ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚

### ã‚¹ãƒ†ãƒƒãƒ—4.5: è¦ç´„ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆ4ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ§‹æˆï¼‰

article-fetcher ãŒç”Ÿæˆã™ã‚‹è¦ç´„ã¯ä»¥ä¸‹ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¾“ã„ã¾ã™:

```markdown
### æ¦‚è¦
- [ä¸»è¦äº‹å®Ÿã‚’ç®‡æ¡æ›¸ãã§3è¡Œç¨‹åº¦]
- [æ•°å€¤ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°å«ã‚ã‚‹]
- [é–¢é€£ä¼æ¥­ãŒã‚ã‚Œã°å«ã‚ã‚‹]

### èƒŒæ™¯
[ã“ã®å‡ºæ¥äº‹ã®èƒŒæ™¯ãƒ»çµŒç·¯ã‚’è¨˜è¼‰ã€‚è¨˜äº‹ã«è¨˜è¼‰ãŒãªã‘ã‚Œã°ã€Œ[è¨˜è¼‰ãªã—]ã€]

### å¸‚å ´ã¸ã®å½±éŸ¿
[æ ªå¼ãƒ»ç‚ºæ›¿ãƒ»å‚µåˆ¸ç­‰ã¸ã®å½±éŸ¿ã‚’è¨˜è¼‰ã€‚è¨˜äº‹ã«è¨˜è¼‰ãŒãªã‘ã‚Œã°ã€Œ[è¨˜è¼‰ãªã—]ã€]

### ä»Šå¾Œã®è¦‹é€šã—
[ä»Šå¾Œäºˆæƒ³ã•ã‚Œã‚‹å±•é–‹ãƒ»æ³¨ç›®ç‚¹ã‚’è¨˜è¼‰ã€‚è¨˜äº‹ã«è¨˜è¼‰ãŒãªã‘ã‚Œã°ã€Œ[è¨˜è¼‰ãªã—]ã€]
```

**é‡è¦ãƒ«ãƒ¼ãƒ«**:
- å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«ã¤ã„ã¦ã€**è¨˜äº‹å†…ã«è©²å½“ã™ã‚‹æƒ…å ±ãŒãªã‘ã‚Œã°ã€Œ[è¨˜è¼‰ãªã—]ã€ã¨è¨˜è¿°**ã™ã‚‹
- æƒ…å ±ã‚’æ¨æ¸¬ãƒ»å‰µä½œã—ã¦ã¯ã„ã‘ãªã„
- è¨˜äº‹ã«æ˜ç¤ºçš„ã«æ›¸ã‹ã‚Œã¦ã„ã‚‹å†…å®¹ã®ã¿ã‚’è¨˜è¼‰ã™ã‚‹

| ã‚»ã‚¯ã‚·ãƒ§ãƒ³ | å†…å®¹ | è¨˜è¼‰ãªã—ã®ä¾‹ |
|-----------|------|-------------|
| æ¦‚è¦ | ä¸»è¦äº‹å®Ÿã€æ•°å€¤ãƒ‡ãƒ¼ã‚¿ | ï¼ˆå¸¸ã«ä½•ã‹è¨˜è¼‰ã§ãã‚‹ã¯ãšï¼‰ |
| èƒŒæ™¯ | çµŒç·¯ã€åŸå› ã€ã“ã‚Œã¾ã§ã®æµã‚Œ | é€Ÿå ±ã§èƒŒæ™¯èª¬æ˜ãŒãªã„å ´åˆ |
| å¸‚å ´ã¸ã®å½±éŸ¿ | æ ªä¾¡ãƒ»ç‚ºæ›¿ãƒ»å‚µåˆ¸ã¸ã®å½±éŸ¿ | å½±éŸ¿ã®è¨€åŠãŒãªã„å ´åˆ |
| ä»Šå¾Œã®è¦‹é€šã— | äºˆæƒ³ã€ã‚¢ãƒŠãƒªã‚¹ãƒˆè¦‹è§£ | å°†æ¥äºˆæ¸¬ã®è¨€åŠãŒãªã„å ´åˆ |

### ã‚¹ãƒ†ãƒƒãƒ—4.6: article-fetcher ã®è©³ç´°ä»•æ§˜

ã‚µãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è©³ç´°ãªå®Ÿè£…ã«ã¤ã„ã¦ã¯ä»¥ä¸‹ã‚’å‚ç…§:
`.claude/agents/news-article-fetcher.md`

**article-fetcher å†…éƒ¨ã§ã®å‡¦ç†ï¼ˆå„è¨˜äº‹ã«å¯¾ã—ã¦ï¼‰**:
1. ãƒšã‚¤ã‚¦ã‚©ãƒ¼ãƒ«/JSäº‹å‰ãƒã‚§ãƒƒã‚¯ï¼ˆ`article_content_checker.py` å‘¼ã³å‡ºã—ï¼‰
2. ãƒã‚§ãƒƒã‚¯é€šéæ™‚: WebFetchã§æœ¬æ–‡å–å¾—
3. 4ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ§‹æˆã®æ—¥æœ¬èªè¦ç´„ã‚’ç”Ÿæˆ
4. è‹±èªã‚¿ã‚¤ãƒˆãƒ«ã‚’æ—¥æœ¬èªã«ç¿»è¨³
5. Issueä½œæˆï¼ˆ`gh issue create` + closeï¼‰-- `.github/ISSUE_TEMPLATE/news-article.yml` æº–æ‹ 
6. Projectè¿½åŠ ï¼ˆ`gh project item-add`ï¼‰
7. Statusè¨­å®šï¼ˆGraphQL APIï¼‰
8. å…¬é–‹æ—¥æ™‚è¨­å®šï¼ˆGraphQL APIï¼‰
9. ãƒã‚§ãƒƒã‚¯ä¸é€šéæ™‚: `skipped` ã«è¨˜éŒ²ã—ã‚¹ã‚­ãƒƒãƒ—

> **é‡è¦ãªå¤‰æ›´**: WebFetchå¤±æ•—æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¦ç´„ç”Ÿæˆï¼ˆRSS summaryãƒ™ãƒ¼ã‚¹ï¼‰ã¯**å»ƒæ­¢**ã€‚
> æœ¬æ–‡ãŒå–å¾—ã§ããªã„è¨˜äº‹ã®è¦ç´„ã¯å“è³ªãŒæ‹…ä¿ã§ããªã„ãŸã‚ã€Issueä½œæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ã€‚

## Phase 5: çµæœå ±å‘Š

### çµ±è¨ˆã‚µãƒãƒªãƒ¼å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã€å¿…é ˆã€‘

> **ğŸš¨ é‡è¤‡ä»¶æ•°ã®å‡ºåŠ›ã¯å¿…é ˆã§ã™ ğŸš¨**
>
> å‡¦ç†çµ±è¨ˆã«ã¯å¿…ãšã€Œé‡è¤‡ã€ã®ä»¶æ•°ã‚’å«ã‚ã¦ãã ã•ã„ã€‚
> ã“ã‚Œã«ã‚ˆã‚Šã€ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ãŒå…¨ãƒ†ãƒ¼ãƒã®é‡è¤‡ä»¶æ•°ã‚’é›†è¨ˆã§ãã¾ã™ã€‚

```markdown
## {theme_name} ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†å®Œäº†

### å‡¦ç†çµ±è¨ˆ

| é …ç›® | ä»¶æ•° |
|------|------|
| å‡¦ç†è¨˜äº‹æ•° | {stats["processed"]} |
| å…¬é–‹æ—¥æ™‚ãƒ•ã‚£ãƒ«ã‚¿é™¤å¤– | {stats["date_filtered"]} |
| ãƒ†ãƒ¼ãƒãƒãƒƒãƒ | {stats["matched"]} |
| **é‡è¤‡ã‚¹ã‚­ãƒƒãƒ—** | **{stats["duplicates"]}** |
| URLãªã—ã‚¹ã‚­ãƒƒãƒ— | {stats["skipped_no_url"]} |
| æ–°è¦æŠ•ç¨¿ | {stats["created"]} |
| æŠ•ç¨¿å¤±æ•— | {stats["failed"]} |

### æŠ•ç¨¿ã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹

| Issue # | ã‚¿ã‚¤ãƒˆãƒ« | å…¬é–‹æ—¥ |
|---------|----------|--------|
{{#created_issues}}
| #{{issue_number}} | {{title}} | {{published_date}} |
{{/created_issues}}

{{#has_duplicates}}
### é‡è¤‡ã§ã‚¹ã‚­ãƒƒãƒ—ã•ã‚ŒãŸè¨˜äº‹

| è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ« | é‡è¤‡å…ˆ | ç†ç”± |
|-------------|--------|------|
{{#duplicate_articles}}
| {{title}} | #{{existing_issue}} | {{reason}} |
{{/duplicate_articles}}
{{/has_duplicates}}
```

### çµæœå ±å‘Šã®å®Ÿè£…ä¾‹

```python
def generate_result_report(
    theme_name: str,
    stats: dict,
    created_issues: list[dict],
    duplicate_articles: list[dict],
) -> str:
    """çµæœãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã™ã‚‹

    Parameters
    ----------
    theme_name : str
        ãƒ†ãƒ¼ãƒåï¼ˆæ—¥æœ¬èªï¼‰
    stats : dict
        çµ±è¨ˆã‚«ã‚¦ãƒ³ã‚¿
    created_issues : list[dict]
        ä½œæˆã—ãŸIssueã®ãƒªã‚¹ãƒˆ
    duplicate_articles : list[dict]
        é‡è¤‡ã§ã‚¹ã‚­ãƒƒãƒ—ã—ãŸè¨˜äº‹ã®ãƒªã‚¹ãƒˆ

    Returns
    -------
    str
        Markdownå½¢å¼ã®ãƒ¬ãƒãƒ¼ãƒˆ
    """

    report = f"""## {theme_name} ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†å®Œäº†

### å‡¦ç†çµ±è¨ˆ

| é …ç›® | ä»¶æ•° |
|------|------|
| å‡¦ç†è¨˜äº‹æ•° | {stats["processed"]} |
| å…¬é–‹æ—¥æ™‚ãƒ•ã‚£ãƒ«ã‚¿é™¤å¤– | {stats["date_filtered"]} |
| ãƒ†ãƒ¼ãƒãƒãƒƒãƒ | {stats["matched"]} |
| **é‡è¤‡ã‚¹ã‚­ãƒƒãƒ—** | **{stats["duplicates"]}** |
| URLãªã—ã‚¹ã‚­ãƒƒãƒ— | {stats["skipped_no_url"]} |
| æ–°è¦æŠ•ç¨¿ | {stats["created"]} |
| æŠ•ç¨¿å¤±æ•— | {stats["failed"]} |

### æŠ•ç¨¿ã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹

| Issue # | ã‚¿ã‚¤ãƒˆãƒ« | å…¬é–‹æ—¥ |
|---------|----------|--------|
"""

    for issue in created_issues:
        report += f"| #{issue['number']} | {issue['title']} | {issue['published_date']} |\n"

    # é‡è¤‡è¨˜äº‹ã®è©³ç´°ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    if duplicate_articles:
        report += f"""
### é‡è¤‡ã§ã‚¹ã‚­ãƒƒãƒ—ã•ã‚ŒãŸè¨˜äº‹ï¼ˆ{len(duplicate_articles)}ä»¶ï¼‰

| è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ« | é‡è¤‡å…ˆ | ç†ç”± |
|-------------|--------|------|
"""
        for dup in duplicate_articles:
            report += f"| {dup['title'][:50]}... | #{dup['existing_issue']} | {dup['reason']} |\n"

    return report
```

## ãƒ†ãƒ¼ãƒåˆ¥Status IDä¸€è¦§

| ãƒ†ãƒ¼ãƒ | Statuså | Option ID |
|--------|----------|-----------|
| index | Index | `3925acc3` |
| stock | Stock | `f762022e` |
| sector | Sector | `48762504` |
| macro | Macro Economics | `730034a5` |
| ai | AI | `6fbb43d0` |
| finance | Finance | `ac4a91b1` |

## GitHub Projectãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ä¸€è¦§

| ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å | ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ID | å‹ | ç”¨é€” |
|-------------|-------------|-----|------|
| Status | `PVTSSF_lAHOBoK6AM4BMpw_zg739ZE` | SingleSelect | ãƒ†ãƒ¼ãƒåˆ†é¡ |
| å…¬é–‹æ—¥æ™‚ | `PVTF_lAHOBoK6AM4BMpw_zg8BzrI` | Date | ã‚½ãƒ¼ãƒˆç”¨ |

## å…±é€šã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

### E001: MCPãƒ„ãƒ¼ãƒ«æ¥ç¶šã‚¨ãƒ©ãƒ¼

```python
def handle_mcp_error(feed_id: str, feed_title: str, error: Exception) -> list[dict]:
    """MCPãƒ„ãƒ¼ãƒ«æ¥ç¶šå¤±æ•—æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†

    Parameters
    ----------
    feed_id : str
        ãƒ•ã‚£ãƒ¼ãƒ‰ID
    feed_title : str
        ãƒ•ã‚£ãƒ¼ãƒ‰åï¼ˆãƒ­ã‚°ç”¨ï¼‰
    error : Exception
        ç™ºç”Ÿã—ãŸã‚¨ãƒ©ãƒ¼

    Returns
    -------
    list[dict]
        ãƒ­ãƒ¼ã‚«ãƒ«ã‹ã‚‰å–å¾—ã—ãŸè¨˜äº‹ï¼ˆå–å¾—ã§ããªã„å ´åˆã¯ç©ºãƒªã‚¹ãƒˆï¼‰
    """

    ãƒ­ã‚°å‡ºåŠ›: f"è­¦å‘Š: MCPãƒ„ãƒ¼ãƒ«æ¥ç¶šå¤±æ•—: {feed_title}"
    ãƒ­ã‚°å‡ºåŠ›: f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {error}"
    ãƒ­ã‚°å‡ºåŠ›: "ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’è©¦è¡Œã—ã¾ã™"

    # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å–å¾—ã‚’è©¦ã¿ã‚‹
    return load_from_local(feed_id, feed_title)
```

### E002: Issueä½œæˆã‚¨ãƒ©ãƒ¼

```python
try:
    result = subprocess.run(
        ["gh", "issue", "create", ...],
        capture_output=True,
        text=True,
        check=True
    )
except subprocess.CalledProcessError as e:
    ãƒ­ã‚°å‡ºåŠ›: f"è­¦å‘Š: Issueä½œæˆå¤±æ•—: {item['title']}"
    ãƒ­ã‚°å‡ºåŠ›: f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {e.stderr}"

    if "rate limit" in str(e.stderr).lower():
        ãƒ­ã‚°å‡ºåŠ›: "GitHub API ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚1æ™‚é–“å¾…æ©Ÿã—ã¦ãã ã•ã„ã€‚"

    failed += 1
    continue
```

### E003: å…¬é–‹æ—¥æ™‚è¨­å®šã‚¨ãƒ©ãƒ¼

```python
try:
    result = subprocess.run(
        ["gh", "api", "graphql", "-f", f"query={mutation}"],
        capture_output=True,
        text=True,
        check=True
    )
except subprocess.CalledProcessError as e:
    ãƒ­ã‚°å‡ºåŠ›: f"è­¦å‘Š: å…¬é–‹æ—¥æ™‚è¨­å®šå¤±æ•—: Issue #{issue_number}"
    ãƒ­ã‚°å‡ºåŠ›: f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {e.stderr}"
    ãƒ­ã‚°å‡ºåŠ›: "Issueä½œæˆã¯æˆåŠŸã—ã¦ã„ã¾ã™ã€‚æ‰‹å‹•ã§å…¬é–‹æ—¥æ™‚ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚"
    continue
```

## å‚è€ƒè³‡æ–™

- **Issueãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ**: `.github/ISSUE_TEMPLATE/news-article.yml`ï¼ˆYAMLå½¢å¼ã€GitHub UIç”¨ï¼‰
- **ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼**: `.claude/agents/finance-news-orchestrator.md`
- **ã‚³ãƒãƒ³ãƒ‰**: `.claude/commands/collect-finance-news.md`
- **GitHub Project**: https://github.com/users/YH-05/projects/15
